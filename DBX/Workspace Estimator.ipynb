{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fb9227-a269-47a2-a7ff-d82f82d5cfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Workspace Estimator\n",
    "\n",
    "To understand your current workload in Databricks, we need to gather some basic information that includes:\n",
    "- Jobs, average monthly running frequency, average running time steps.\n",
    "- Tasks within each job, notebooks of each step.\n",
    "- Job Categories: Data Engineering, Machine Learning, or Streaming.\n",
    "- Cluster information [Size, type]\n",
    "\n",
    "\\*All this information will be generated using Databrick Python API, and it will rest in your account for your perusal after that, if you agree, we will provide the steps to share that information with us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "17d869b2-e741-4f46-bc4b-0a3aaec96ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Requirements\n",
    "1. Databricks Host (should begin with https://). <br> \n",
    "example: https://demo.cloud.databricks.com <br>\n",
    "for more details visit [workspace details](https://docs.databricks.com/workspace/workspace-details.html)\n",
    "2. token. For more details [Databricks Authentication](https://docs.databricks.com/dev-tools/api/latest/authentication.html)\n",
    "\n",
    "### Permissions\n",
    "The workspace admin account will required the following permissions:\n",
    "- Personal Access Tokens\n",
    "- Workspace visibility Control\n",
    "- Cluster Visibility Control\n",
    "- Job Visibility Control\n",
    "- DBS File Browser\n",
    "### Workflows/Jobs Requirements\n",
    "To be able to make an estimation, each jobs need to have:\n",
    "- A scheduling configured \n",
    "- At least one successful execution in the last 60 days. \n",
    "- If runnning in staging or production mirror, it must include\n",
    "  - Same machine configuration\n",
    "  - Complete dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8261f957-f438-40f2-a441-867c0a2311a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "333b2f8e-89aa-4957-b47b-49663f432c0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d75ed6-06bc-41a3-9a25-2f7941427e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from urllib.parse import urlencode\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "class NoClusterEventsError(Exception):\n",
    "    def __init__(self, message=None):\n",
    "        self.message = message\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "\n",
    "class Manager:\n",
    "    results_count = {}\n",
    "    def __init__(self, input_url=None, input_token=None, input_output=\"./output\"):\n",
    "        if input_url:\n",
    "            input_url = input_url[:-1] if input_url.endswith('/') else input_url\n",
    "        self.url = input_url\n",
    "        self.token = input_token\n",
    "        self.output = input_output\n",
    "        self.api_utl = Util()\n",
    "        os.makedirs(self.output, exist_ok=True)\n",
    "\n",
    "    def show_results(self, days):\n",
    "        print(f\"The following information was collected by the Workspace Estimator for the last {days} days\")\n",
    "        for key, value in self.results_count.items():\n",
    "            print(f\"{key.upper().ljust(10)}:\\t{value}\")\n",
    "\n",
    "    def generator(self):\n",
    "        while True:\n",
    "            yield\n",
    "\n",
    "    def get_and_save(self, path=None, name_output=\"default_output\", array_field=None, suffix=\"\", default_params=None,\n",
    "                     use_paging=False, post=False, body=None, url_api=\"\", cloud_provider=\"\", pb=None,pb_message=None, paging_pb=False, full_response=False):\n",
    "        has_more = True\n",
    "        counter = 0\n",
    "        next_page_token = \"\"\n",
    "        new_url = \"\"\n",
    "        response = []\n",
    "        query = \"\"\n",
    "        paging = {}\n",
    "        offset = -1\n",
    "        full_json = []\n",
    "        if body is None:\n",
    "            body = {}\n",
    "        if default_params is None:\n",
    "            default_params = {}\n",
    "        skip = 0\n",
    "        if pb:\n",
    "              pb.set_description(f\"{pb_message}\") if pb_message else pb.set_description(f\"Processing {path}\")\n",
    "        try:\n",
    "            new_params = default_params.copy()\n",
    "            pb_paging = None\n",
    "            file_output = f\"{name_output}{suffix}\"\n",
    "            gen = self.generator()\n",
    "            if paging_pb:\n",
    "                pb_paging = tqdm_notebook(gen, desc=f\"Pages in {path}\")\n",
    "            for _ in gen:\n",
    "                new_params = self.api_utl.get_params(counter, new_params, next_page_token, offset, use_paging, skip)\n",
    "                response = self.get_response(body, new_params, path, post, url_api)\n",
    "                json_data = self.api_utl.get_full_json(array_field, full_json, name_output, path, response, full_response, cloud_provider=cloud_provider)\n",
    "                paging = self.api_utl.get_paging(json_data)\n",
    "                offset = self.api_utl.get_offset(json_data, offset)\n",
    "                skip = paging.get('has_skip')\n",
    "                has_more = self.api_utl.get_has_more(json_data, offset)\n",
    "                next_page_token = self.api_utl.get_page_token(paging)\n",
    "                counter += 1\n",
    "                if paging_pb:\n",
    "                    pb_paging.update(1)\n",
    "                if not has_more:\n",
    "                    break\n",
    "            if paging_pb:\n",
    "                pb_paging.close()\n",
    "            Util.write_file_request_(self.output, file_output, full_json)\n",
    "            Util.check_file_request_(self.output, file_output, full_json)\n",
    "        except Exception as e:\n",
    "            local_vars = locals().copy()\n",
    "            Util.write_log(self.output, e, local_vars)\n",
    "\n",
    "        if pb:\n",
    "            pb.update(1)\n",
    "        self.results_count[name_output] = len(full_json)\n",
    "\n",
    "    def get_response(self, body, new_params, path, post, url):\n",
    "        query = self.api_utl.get_query(new_params)\n",
    "        url_path = f\"{url}/{path}\" if path else url\n",
    "        url_not_query = url_path if path else url\n",
    "        new_url = f\"{url_path}?{query}\" if query else url_not_query\n",
    "        headers = {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "        if post:\n",
    "            response = requests.post(new_url, headers=headers, json=body)\n",
    "        else:\n",
    "            response = requests.get(new_url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            error = f\"Failed connection - {response.content}\"\n",
    "            if \"does not exist\" in error:\n",
    "                raise NoClusterEventsError(response.content)\n",
    "            raise Exception(error)\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Mapping:\n",
    "    @staticmethod\n",
    "    def get_clusters_ids(output=None):\n",
    "        results = []\n",
    "        results.extend(Mapping.get_clusters_ids_from_runs(output))\n",
    "        results.extend(Mapping.get_clusters_ids_from_clusters(output))\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def get_clusters_ids_from_runs(output=None):\n",
    "        if not output:\n",
    "            return []\n",
    "        runs_list = Mapping.get_runs(output)\n",
    "        if len(runs_list) > 0:\n",
    "            result_df = pd.DataFrame(runs_list)\n",
    "            result_df['duration'] = result_df['end_time'] - result_df['start_time']\n",
    "            result_df = result_df[(result_df['duration'] > 0) & (result_df['result_state'] == 'SUCCESS')]\n",
    "            result_df['rank'] = result_df.groupby(['run_name'])['end_time'].rank('first', ascending=False)\n",
    "            rank_first_df = result_df[result_df['rank'] == 1]\n",
    "            rank_first_df = rank_first_df.dropna(subset=['cluster_id'])\n",
    "            cluster_ids = rank_first_df['cluster_id'].values\n",
    "            return set(cluster_ids)\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_clusters_ids_from_clusters(output=None):\n",
    "        if not output:\n",
    "            return []\n",
    "        cluster_list = Mapping.get_clusters(output)\n",
    "        if len(cluster_list) > 0:\n",
    "            new_clusters = []\n",
    "            result_df = pd.DataFrame(cluster_list)\n",
    "            ui_clusters = result_df[result_df['cluster_source'] != 'JOB']['cluster_id'].values\n",
    "            new_clusters.extend(set(ui_clusters))\n",
    "            result_df['duration'] = result_df['end_time'] - result_df['start_time']\n",
    "            result_df = result_df[(result_df['duration'] > 0) & (result_df['cluster_source'] == 'JOB') & (result_df['result_state'] == 'SUCCESS')]\n",
    "            result_df['rank'] = result_df.groupby(['run_name'])['end_time'].rank('first', ascending=False)\n",
    "            rank_first_df = result_df[result_df['rank'] == 1]\n",
    "            rank_first_df = rank_first_df.dropna(subset=['cluster_id'])\n",
    "            cluster_ids = rank_first_df['cluster_id'].values\n",
    "            new_clusters.extend(set(cluster_ids))\n",
    "            return new_clusters\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_runs_ids(output=None):\n",
    "        if not output:\n",
    "            return []\n",
    "        runs_list = Mapping.get_runs(output)\n",
    "        if len(runs_list) > 0:\n",
    "            result_df = pd.DataFrame(runs_list)\n",
    "            result_df['duration'] = result_df['end_time'] - result_df['start_time']\n",
    "            result_df = result_df[(result_df['duration'] > 0) & (result_df['result_state'] == 'SUCCESS')]\n",
    "            result_df['rank'] = result_df.groupby(['run_name'])['end_time'].rank('first', ascending=False)\n",
    "            rank_first_df = result_df[result_df['rank'] == 1]\n",
    "            rank_first_df = rank_first_df.dropna(subset=['run_id'])\n",
    "            runs_ids = rank_first_df['run_id'].values\n",
    "            return set(runs_ids)\n",
    "        return []\n",
    "\n",
    "    @staticmethod\n",
    "    def get_clusters(output):\n",
    "        result = []\n",
    "        files = glob.glob(os.path.join(output, 'clusters*.json'))\n",
    "        for f in files:\n",
    "            with open(f, 'r') as file:\n",
    "                clusters = json.load(file)\n",
    "                for cluster in clusters:\n",
    "                    cluster_id = cluster.get(\"cluster_id\", None) if cluster else None\n",
    "                    cluster_source = cluster.get(\"cluster_source\", None) if cluster else None\n",
    "                    tags = cluster.get(\"default_tags\", None) if cluster else None\n",
    "                    job_id = tags.get(\"JobId\", \"NO_ID_FOUND\") if tags else \"NO_TAG_FOUND\"\n",
    "                    run_name = tags.get(\"RunName\", \"NO_NAME_FOUND\") if tags else \"NO_TAG_FOUND\"\n",
    "                    start_time = cluster.get(\"start_time\", 0)\n",
    "                    end_time = cluster.get(\"end_time\", 0)\n",
    "                    termination_reason = cluster.get(\"termination_reason\")\n",
    "                    result_state = termination_reason.get(\"type\") if termination_reason else None\n",
    "                    result.append({\n",
    "                        \"cluster_id\": cluster_id,\n",
    "                        \"cluster_source\": cluster_source,\n",
    "                        \"run_name\": Util.get_clean_name(run_name),\n",
    "                        \"job_id\": job_id,\n",
    "                        \"start_time\": start_time,\n",
    "                        \"end_time\": end_time,\n",
    "                        \"result_state\": result_state\n",
    "                    })\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def get_runs(output):\n",
    "        result = []\n",
    "        files = glob.glob(os.path.join(output, 'run*.json'))\n",
    "        for f in files:\n",
    "            with open(f, 'r') as file:\n",
    "                runs = json.load(file)\n",
    "                for run in runs:\n",
    "                    run_id = run.get(\"run_id\", None) if run else None\n",
    "                    run_name = run.get(\"run_name\", None) if run else None\n",
    "                    start_time = run.get(\"start_time\", None) if run else None\n",
    "                    end_time = run.get(\"end_time\", None) if run else None\n",
    "                    if \"tasks\" in run:\n",
    "                        tasks = run[\"tasks\"]\n",
    "                        for task in tasks:\n",
    "                            existing_cluster_id = task.get(\"existing_cluster_id\", None)\n",
    "                            cluster_instance = task.get(\"cluster_instance\", None)\n",
    "                            cluster_id = cluster_instance.get(\"cluster_id\",\n",
    "                                                              None) if cluster_instance else existing_cluster_id\n",
    "                            state = task.get(\"state\", None)\n",
    "                            result_state = state.get(\"result_state\", None) if state else None\n",
    "                            run_name = run_name[:-37] if 'ADF' in run_name else run_name\n",
    "                            result.append({\n",
    "                                \"run_name\": Util.get_clean_name(run_name),\n",
    "                                \"run_id\": run_id,\n",
    "                                \"start_time\": start_time,\n",
    "                                \"end_time\": end_time,\n",
    "                                \"cluster_instance\": cluster_instance,\n",
    "                                \"cluster_id\": cluster_id,\n",
    "                                \"result_state\": result_state\n",
    "                            })\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class Sizing(Manager):\n",
    "    def __init__(self, input_url, input_token=None, input_output=\"./output\"):\n",
    "        super().__init__(input_url, input_token=input_token, input_output=input_output)\n",
    "        self.token = input_token\n",
    "        self.output = input_output\n",
    "        os.makedirs(self.output, exist_ok=True)\n",
    "\n",
    "    def get_clusters_events(self, timestamp, pb=None):\n",
    "        events_path = \"api/2.0/clusters/events\"\n",
    "        if pb:\n",
    "            pb.set_description(f\"Processing {events_path}\")\n",
    "\n",
    "        cluster_list = Mapping.get_clusters_ids(self.output)\n",
    "        with tqdm_notebook(range(len(cluster_list)), desc=\"Fetching Cluster Events\") as pb2:\n",
    "            for cluster in cluster_list:\n",
    "                self.get_and_save(path=events_path, name_output=f\"events\", suffix=f\"_{cluster}\", post=True,\n",
    "                                  default_params={\"cluster_id\": f\"{cluster}\", \"limit\": 250, \"start_time\": timestamp},\n",
    "                                  body={\"event_types\": (\n",
    "                                      \"CREATING\", \"STARTING\", \"RESTARTING\", \"TERMINATING\", \"RUNNING\", \"RESIZING\",\n",
    "                                      \"UPSIZE_COMPLETED\", \"EDITED\")}, use_paging=True, url_api=self.url,\n",
    "                                  pb=pb2, pb_message=f\"Fetching {cluster} events\")\n",
    "        if pb:\n",
    "            pb.update(1)\n",
    "\n",
    "    def get_runs_details(self, pb=None):\n",
    "        runs_details_path = \"api/2.1/jobs/runs/get\"\n",
    "        if pb:\n",
    "            pb.set_description(f\"Processing {runs_details_path}\")\n",
    "\n",
    "        runs_list = Mapping.get_runs_ids(self.output)\n",
    "        with tqdm_notebook(range(len(runs_list)), desc=\"Fetching Runs Details\") as pb2:\n",
    "            for run_id in runs_list:\n",
    "                self.get_and_save(path=runs_details_path, name_output=f\"runs_details\", suffix=f\"_{run_id}\",\n",
    "                                  default_params={\"run_id\": run_id}, url_api=self.url,\n",
    "                                  pb=pb2, pb_message=f\"Fetching details of run:{run_id}\", full_response=True)\n",
    "        if pb:\n",
    "            pb.update(1)\n",
    "\n",
    "    def get_metadata(self, days=60):\n",
    "        with tqdm_notebook(range(9), desc='Processing...') as pb:\n",
    "            date_days_ago = datetime.now() - timedelta(days=days)\n",
    "            timestamp = int(date_days_ago.timestamp() * 1000)\n",
    "            self.get_and_save(path=\"api/2.0/clusters/list-node-types\", name_output=\"node_types\", url_api=self.url,\n",
    "                              pb=pb)\n",
    "            self.get_and_save(path=\"api/2.0/clusters/list\", name_output=\"clusters\", use_paging=True, url_api=self.url,\n",
    "                              pb=pb)\n",
    "            self.get_and_save(path=\"api/2.0/jobs/list\", name_output=\"jobs\", use_paging=True, url_api=self.url, pb=pb)\n",
    "            self.get_and_save(path=\"api/2.1/jobs/runs/list\", name_output=\"runs\", use_paging=True,\n",
    "                              default_params={\"expand_tasks\": \"true\", \"start_time_from\": timestamp}, url_api=self.url,\n",
    "                              pb=pb, paging_pb=True)\n",
    "            self.get_and_save(path=\"api/2.0/sql/warehouses\", name_output=\"warehouses\", use_paging=True,\n",
    "                              url_api=self.url, pb=pb)\n",
    "            self.get_and_save(path=\"api/2.0/pipelines\", name_output=\"pipelines\", array_field=\"statuses\",\n",
    "                              use_paging=True,\n",
    "                              default_params={\"max_results\": 100}, url_api=self.url, pb=pb)\n",
    "            self.get_and_save(path=\"api/2.0/sql/history/queries\", name_output=\"queries\", array_field=\"res\", use_paging=True,\n",
    "                              url_api=self.url, pb=pb)\n",
    "            self.get_clusters_events(timestamp, pb=pb)\n",
    "            self.get_runs_details(pb=pb)\n",
    "\n",
    "\n",
    "\n",
    "class UtilFile:\n",
    "    email_pattern = re.compile(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}')\n",
    "    url_parameters_pattern = re.compile('^(?:https?:\\/\\/)?(?:[^@\\/\\n]+@)?(?:www\\.)?([^:?\\n]+)')\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    jwt_pattern = re.compile(r'[A-Za-z0-9_-]{4,}(?:\\.[A-Za-z0-9_-]{4,}){2}')\n",
    "    dbx_pattern = re.compile(r'https?://adb-\\d{4,16}\\.\\d{0,2}|https?://dbc-.{4,12}-.{2,4}')\n",
    "\n",
    "    @staticmethod\n",
    "    def check_file_request_(output, name_output, json_data_check):\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "        file_path = os.path.join(output, f\"{name_output}.json\")\n",
    "        if os.path.exists(file_path):\n",
    "            size = os.path.getsize(file_path)\n",
    "            size_max_unit_value = \"10 MB\"\n",
    "            array_units = (\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n",
    "            size_bites_unit_value = UtilFile.convert_size_to_mb(size)\n",
    "            size_correct = UtilFile.has_correct_size(size_bites_unit_value, size_max_unit_value, array_units)\n",
    "            if size_correct:\n",
    "                return True\n",
    "            else:\n",
    "                current_size_in_mb = UtilFile.convert_size_to_mb(size)\n",
    "                size_value = int(current_size_in_mb.split(' ')[0])\n",
    "                size_max_value = int(size_max_unit_value.split(' ')[0])\n",
    "                parts = math.ceil(size_value / size_max_value)\n",
    "                count = UtilFile.get_count(json_data_check)\n",
    "                size_part = math.ceil(count / parts)\n",
    "                end = 0\n",
    "                for i in range(parts):\n",
    "                    start = end + 1 if end != 0 else 0\n",
    "                    end = end + size_part if i != parts else count - 1\n",
    "                    json_data_part = json_data_check[start:end]\n",
    "                    # the following line is to activate the cleanup of the json files.\n",
    "                    # json_data_part = UtilFile.filter_data(json_data_part, [])\n",
    "                    UtilFile.write_file_request_(output, f\"{name_output}_{i + 1:02d}\", json_data_part)\n",
    "                file_path = os.path.join(output, f\"{name_output}.json\")\n",
    "                os.remove(file_path)\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_size_to_mb(size_bytes):\n",
    "        if size_bytes == 0:\n",
    "            return \"0 MB\"\n",
    "        i = int(math.floor(math.log(size_bytes, 1024)))\n",
    "        p = math.pow(1024, 2)\n",
    "        s = int(round(size_bytes / p, 0))\n",
    "        return \"%s %s\" % (s, 'MB')\n",
    "\n",
    "    @staticmethod\n",
    "    def compress_folder_to_zip(output, output_zip_file, extension='zip'):\n",
    "        try:\n",
    "\n",
    "            if not os.path.exists(output):\n",
    "                raise Exception(f\"The source folder '{output}' does not exist.\")\n",
    "\n",
    "            with zipfile.ZipFile(f\"{output_zip_file}.{extension}\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "                for root, dirs, files in os.walk(output):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        relative_path = os.path.relpath(file_path, output)\n",
    "                        zipf.write(file_path, relative_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_output(output):\n",
    "        shutil.rmtree(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_count(json_data_count):\n",
    "        if isinstance(json_data_count, list):\n",
    "            count = len(json_data_count)\n",
    "        elif isinstance(json_data_count, dict):\n",
    "            count = len(json_data_count)\n",
    "        else:\n",
    "            count = 0\n",
    "        return count\n",
    "\n",
    "    @staticmethod\n",
    "    def get_file_name(workspace):\n",
    "        workspace = workspace.replace(\" \", \"_\")\n",
    "        workspace = workspace.replace(\" \", \"\")\n",
    "        workspace = workspace.replace(\" \", \"_\")\n",
    "        workspace = re.sub(r'[^A-Za-z0-9 ]+', '', workspace)\n",
    "        workspace = \"Default_wkp\" if workspace == \"\" else workspace\n",
    "        now = datetime.now()\n",
    "        date_part = now.strftime('%m%d')\n",
    "        name = f\"{workspace}_{date_part}\"\n",
    "        return name\n",
    "\n",
    "    @staticmethod\n",
    "    def has_correct_size(size_bites_unit_value, size_max_unit_value, array_units):\n",
    "        size_unit = size_bites_unit_value.split(' ')[1]\n",
    "        size_index = array_units.index(size_unit)\n",
    "        size_max_unit = size_max_unit_value.split(' ')[1]\n",
    "        size_max_index = array_units.index(size_max_unit)\n",
    "        if size_index < size_max_index:\n",
    "            return True\n",
    "        elif size_index > size_max_index:\n",
    "            return False\n",
    "        else:\n",
    "            size_value = int(size_bites_unit_value.split(' ')[0])\n",
    "            size_max_value = int(size_max_unit_value.split(' ')[0])\n",
    "            if size_value > size_max_value:\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "\n",
    "    @staticmethod\n",
    "    def read_config(config_path):\n",
    "        with open(config_path, 'r') as file:\n",
    "            config_values = json.load(file)\n",
    "        if 'url' not in config_values:\n",
    "            raise ValueError(\"Missing 'url' in configuration file.\")\n",
    "        if 'token' not in config_values:\n",
    "            raise ValueError(\"Missing 'token' in configuration file.\")\n",
    "        return config_values\n",
    "\n",
    "    @staticmethod\n",
    "    def write_log(output, e, local_vars):\n",
    "        log_path = os.path.join(output, \"log.txt\")\n",
    "        with open(log_path, \"a\") as log_file:\n",
    "            log_file.write(\"An error occurred:\\n\")\n",
    "            log_file.write(f\"{str(e)}\\n\")\n",
    "            if not isinstance(e, NoClusterEventsError):\n",
    "                variables_message = \"Local Variables at the point of exception:\\n\"\n",
    "                items = local_vars.items()\n",
    "                log_file.write(variables_message)\n",
    "                not_include = ['self', 'file', 'e', 'log_file']\n",
    "                variables_info = \"   \".join([f\"{key}: {value}\\n\" for key, value in items if key not in not_include])\n",
    "                log_file.write(f\"   {variables_info}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def write_file_request_(output, name_output, json_data):\n",
    "        data = json.dumps(json_data)\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "        file_path = os.path.join(output, f\"{name_output}.json\")\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(data)\n",
    "\n",
    "    @staticmethod\n",
    "    def filter_data(json_data, keys):\n",
    "        if isinstance(json_data, dict):\n",
    "            new_data = UtilFile.clean_dictionary(json_data, keys, {})\n",
    "        elif isinstance(json_data, list):\n",
    "            new_data = UtilFile.clean_list(json_data, keys, [])\n",
    "        else:\n",
    "            new_data = json_data\n",
    "\n",
    "        return new_data\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_list(source, keys, new_list):\n",
    "        if len(source) == 0:\n",
    "            return new_list\n",
    "        new_data = []\n",
    "        for value in source:\n",
    "            if isinstance(value, str):\n",
    "                new_data.append(UtilFile.clean_str(value))\n",
    "            elif isinstance(value, dict):\n",
    "                new_data.append(UtilFile.clean_dictionary(value, keys, {}))\n",
    "            elif isinstance(value, list):\n",
    "                new_data.append(UtilFile.clean_list(value, keys, []))\n",
    "            else:\n",
    "                new_data.append(value)\n",
    "\n",
    "        return new_data\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_dictionary(source, keys, new_dict):\n",
    "        if len(source) == 0:\n",
    "            return new_dict\n",
    "        for key, value in source.items():\n",
    "            if len(keys) > 0 and key in keys:\n",
    "                continue\n",
    "            elif isinstance(value, str):\n",
    "                new_dict[key] = UtilFile.clean_str(value)\n",
    "            elif isinstance(value, dict):\n",
    "                new_dict[key] = UtilFile.clean_dictionary(value, keys, {})\n",
    "            elif isinstance(value, list):\n",
    "                new_dict[key] = UtilFile.clean_list(value, keys, [])\n",
    "\n",
    "            else:\n",
    "                new_dict[key] = value\n",
    "\n",
    "        return new_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_str(txt):\n",
    "        clean_string = UtilFile.replace_emails(txt)\n",
    "        clean_string = UtilFile.replace_adb_url(clean_string)\n",
    "        clean_string = UtilFile.remove_url_parameters(clean_string)\n",
    "        clean_string = UtilFile.remove_jwt(clean_string)\n",
    "        return clean_string\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_emails(text):\n",
    "        return UtilFile.email_pattern.sub('[EMAIL_REMOVED]', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url_parameters(text):\n",
    "        matches = UtilFile.url_parameters_pattern.match(text)\n",
    "        if matches:\n",
    "            return matches.group(1)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_url(text):\n",
    "        return UtilFile.url_pattern.sub('[URL_REMOVED]', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_adb_url(text):\n",
    "        return UtilFile.dbx_pattern.sub('https://adb-0000000000000.00', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_jwt(txt):\n",
    "        return UtilFile.jwt_pattern.sub('[TOKEN_REMOVED]', txt)\n",
    "\n",
    "\n",
    "class Util(UtilFile):\n",
    "\n",
    "    @staticmethod\n",
    "    def get_url(input_url):\n",
    "        return input_url[:-1] if input_url.endswith('/') else input_url\n",
    "\n",
    "    @staticmethod\n",
    "    def get_full_json(array_field, full_json, name_output, path, response, full_response, cloud_provider=\"\"):\n",
    "        json_data = response.json()\n",
    "        field_out = array_field if array_field else name_output\n",
    "        if field_out in json_data:\n",
    "            if path or cloud_provider == \"azure\":\n",
    "                data = json_data[field_out]\n",
    "            else:\n",
    "                data = json_data[field_out].items()\n",
    "            full_json.extend(data)\n",
    "        if full_response:\n",
    "            if isinstance(json_data, list):\n",
    "                full_json.extend(json_data)\n",
    "            else:\n",
    "                full_json.append(json_data)\n",
    "\n",
    "        return json_data\n",
    "\n",
    "    @staticmethod\n",
    "    def get_has_more(json_data, offset):\n",
    "        return json_data.get('has_more') or json_data.get('has_next_page') or offset or json_data.get('NextPageLink')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_page_token(paging):\n",
    "        return paging.get('next_page_token')\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(counter, new_params, next_page_token, offset, use_paging, skip):\n",
    "        if use_paging and counter > 0:\n",
    "            if next_page_token:\n",
    "                new_params['page_token'] = next_page_token\n",
    "            elif skip:\n",
    "                new_params['$skip'] = skip\n",
    "            elif offset and offset >= 0:\n",
    "                new_params['offset'] = offset\n",
    "        return new_params\n",
    "\n",
    "    @staticmethod\n",
    "    def get_paging(json_data):\n",
    "        return {\n",
    "            'has_more': json_data.get('has_more'),\n",
    "            'next_page_token': json_data.get('next_page_token'),\n",
    "            'has_next_page': json_data.get('has_next_page'),\n",
    "            'has_skip': Util.get_skip(json_data),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_offset(json_data, offset):\n",
    "        next_data = json_data.get('next_page')\n",
    "        offset = json_data.get('next_page')['offset'] if next_data else None\n",
    "        return offset\n",
    "\n",
    "    @staticmethod\n",
    "    def get_query(params):\n",
    "        query = \"\"\n",
    "        return urlencode(params) if params else query\n",
    "\n",
    "    @staticmethod\n",
    "    def get_skip(json_data):\n",
    "        next_page_link = json_data.get('NextPageLink', None)\n",
    "        if next_page_link:\n",
    "            skip = json_data.get('NextPageLink').split('$skip=')[1]\n",
    "            return skip\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_clean_name(run_name):\n",
    "        return run_name[:-37] if run_name.startswith(\"ADF_\") else run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad1038b-473c-4eaa-89fa-e46c9acbafce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configure security settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb76c913-8d9f-4114-a4d0-b47bc5476551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.dropdown(\"runs_for_last_days\", \"60\", [\"15\", \"30\", \"60\"])\n",
    "wkp_name_instructions = \"please write workspace name\"\n",
    "dbutils.widgets.text(\"workspace_name\", wkp_name_instructions)\n",
    "# following line tries to get the host from the current workspace, if that fails you can change it manually to your desired host.\n",
    "# host_name = \"demo.azuredatabricks.net\"\n",
    "host_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().get()\n",
    "url = f\"https://{host_name}/\" if host_name != '' else None\n",
    "if url is None:\n",
    "    raise Exception(\"Please provide the workspace url available in your address bar in the variable 'url' i.e. 'https://demo.azuredatabricks.net/'\")\n",
    "# Following line tries to get the token of the current notebook if that fails you can change it manually as in the example below.\n",
    "# We advise against using an explicit token. Please store it in a secret scope.\n",
    "# token = dbutils.secrets.get(scope='my-secrets', key='workload-query')\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "if token == '' or token is None:\n",
    "    raise Exception(\"Please provide a token to query Databricks API. Please define it in the variable 'token'.\")\n",
    "workspace_name = dbutils.widgets.get('workspace_name')\n",
    "workspace_name = 'workspace_name' if dbutils.widgets.get('workspace_name') == wkp_name_instructions else workspace_name\n",
    "days = int(dbutils.widgets.get('runs_for_last_days'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9683b0a0-9e16-4a4d-8d77-881b5ed5dc01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "api_util = Util()\n",
    "tmp_folder ='%05x' % random.randrange(16**5)\n",
    "tmp_driver = f'file:///tmp/{tmp_folder}/'\n",
    "filename = api_util.get_file_name(workspace_name)\n",
    "driver_zip_filename = f'/tmp/{filename}'\n",
    "driver_to_zip = f'/tmp/{tmp_folder}/'\n",
    "driver_folder = f'file:///tmp/{tmp_folder}' \n",
    "zip_path = f'file:///tmp/{filename}.zip'\n",
    "\n",
    "dbutils.fs.mkdirs(tmp_driver)\n",
    "client = Sizing(url, token, driver_to_zip)\n",
    "client.get_metadata(days)\n",
    "client.show_results(days)\n",
    "shutil.make_archive(driver_zip_filename, 'zip', driver_to_zip)\n",
    "zip_destination = 'dbfs:/FileStore/WAS_Tool/results'\n",
    "dbutils.fs.mkdirs(zip_destination)\n",
    "dbutils.fs.cp(zip_path, zip_destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da93574-d6f1-444e-a9f1-90e78fb18edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display as displayHTML, HTML\n",
    "html = f'<html><div  style=\"display:flex;justify-content: center;\"><a href=/files/WAS_Tool/results/{filename}.zip><button style=\"background-color:#249edc;color: #fff;border:1px solid #249edc;cursor:pointer;border-radius:45px;font-weight:800;line-height:18px;padding: 8px 16px\" type=\"button\">DOWNLOAD ZIP</button></a></div></html>'\n",
    "displayHTML(HTML(html))\n",
    "print(f\"In case the download button was not being displayed, please click on the following link: {url}files/WAS_Tool/results/{filename}.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workspace Estimator (2)",
   "widgets": {
    "runs_for_last_days": {
     "currentValue": "30",
     "nuid": "e6caf6fa-84d2-4554-9781-af758535549c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "60",
      "label": null,
      "name": "runs_for_last_days",
      "options": {
       "choices": [
        "15",
        "30",
        "60"
       ],
       "fixedDomain": true,
       "multiselect": false,
       "widgetDisplayType": "Dropdown"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "60",
      "label": null,
      "name": "runs_for_last_days",
      "options": {
       "autoCreated": null,
       "choices": [
        "15",
        "30",
        "60"
       ],
       "widgetType": "dropdown"
      },
      "widgetType": "dropdown"
     }
    },
    "workspace_name": {
     "currentValue": "please write workspace name",
     "nuid": "53383c9e-183d-4e67-97fb-6d6fed247d74",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "please write workspace name",
      "label": null,
      "name": "workspace_name",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "please write workspace name",
      "label": null,
      "name": "workspace_name",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
