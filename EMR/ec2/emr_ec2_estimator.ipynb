{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e881767c-f2c6-43ea-b25a-0055083b4084",
   "metadata": {},
   "source": [
    "# EMR EC2 Estimator\n",
    "\n",
    "Welcome to the EMR EC2 estimator. Please follow the steps below to get your estimation files.\n",
    "\n",
    "1.  Initializes configuration parameters such as the **AWS region**, **user email**, **company**, and the **data retrieval period**.\n",
    "\n",
    "    **(Optional)** If you want to save your estimation files to S3 instead of downloading them, also set the **S3 configuration**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a8e6d9-4eba-4c9e-8f00-69fe1a3d5a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = \"us-west-2\"\n",
    "email = \"\"\n",
    "company = \"\"\n",
    "runs_for_last_days = 30  # Default 30 days\n",
    "\n",
    "# If you want to save the files in S3, set these values\n",
    "s3_bucket_name = \"\"  # my-bucket\n",
    "s3_folder_name = \"\"  # my_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab8f83-7c8d-4a5a-ad91-329a76b421c5",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "2. Imports necessary Python libraries for data manipulation, AWS interaction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ce4bea-89fe-4abb-8782-281ef720216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import uuid\n",
    "import zipfile\n",
    "\n",
    "from collections.abc import Iterator\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from enum import Enum\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from botocore.config import Config as Boto3Config\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf438ddd-f2d3-4fdf-9a21-7cececa40c95",
   "metadata": {},
   "source": [
    "# Boto3 Configuration\n",
    "\n",
    "3. Sets up a retry policy for boto3 with a maximum of 10 attempts to enhance the resilience of AWS API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557f539-53d9-4e4b-93d9-0e91a2bf69a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Boto3Config(retries={\"max_attempts\": 10, \"mode\": \"standard\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df279142",
   "metadata": {},
   "source": [
    "# Constants\n",
    "\n",
    "4. Defines string constants for AWS data fields and establishes connections to AWS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda92fa3-36ce-4584-80ba-1328d22669d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution details constant\n",
    "\n",
    "NOTEBOOK_VERSION = \"0.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b46ff-f20f-4dd7-a6b9-7382f2c5f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Keys\n",
    "STATUS_KEY = \"status\"\n",
    "STATE_KEY = \"state\"\n",
    "STATE_CHANGE_REASON_KEY = \"state_change_reason\"\n",
    "CODE_KEY = \"code\"\n",
    "CREATION_DATE_TIME_KEY = \"creation_date_time\"\n",
    "READY_DATE_TIME_KEY = \"ready_date_time\"\n",
    "END_DATE_TIME_KEY = \"end_date_time\"\n",
    "CONFIGURATIONS_KEY = \"configurations\"\n",
    "BID_PRICE_KEY = \"bid_price\"\n",
    "EBS_OPTIMIZED_KEY = \"ebs_optimized\"\n",
    "\n",
    "# General Keys\n",
    "CLUSTER_ID_KEY = \"cluster_id\"\n",
    "INSTANCE_ID_KEY = \"instance_id\"\n",
    "INSTANCE_TYPE_KEY = \"instance_type\"\n",
    "DURATION_SECONDS_KEY = \"seconds\"\n",
    "INSTANCE_GROUP_ID_KEY = \"instance_group_id\"\n",
    "INSTANCE_FLEET_ID_KEY = \"instance_fleet_id\"\n",
    "\n",
    "\n",
    "# Cluster Keys\n",
    "NORMALIZED_INSTANCE_HOURS_KEY = \"normalized_instance_hours\"\n",
    "AUTO_TERMINATE_KEY = \"auto_terminate\"\n",
    "TERMINATION_PROTECTED_KEY = \"termination_protected\"\n",
    "PROVISIONING_TIMEOUT_MINUTES_KEY = \"provisioning_timeout_minutes\"\n",
    "RELEASE_LABEL_KEY = \"release_label\"\n",
    "SCALE_DOWN_BEHAVIOR_KEY = \"scale_down_behavior\"\n",
    "STEP_CONCURRENCY_LEVEL_KEY = \"step_concurrency_level\"\n",
    "EBS_ROOT_VOLUME_SIZE_KEY = \"ebs_root_volume_size\"\n",
    "OS_RELEASE_LABEL_KEY = \"os_release_label\"\n",
    "EC2_AVAILABILITY_ZONE_KEY = \"ec2_availability_zone\"\n",
    "APPLICATIONS_KEY = \"applications\"\n",
    "MANAGED_SCALING_POLICY_KEY = \"managed_scaling_policy\"\n",
    "BOOTSTRAP_ACTIONS_KEY = \"bootstrap_actions\"\n",
    "REDUCTIONS_KEY = \"reductions\"\n",
    "MANAGED_SCALING_JOINING_TIMEOUT_MINUTES_KEY = \"managed_scaling_joining_timeout_minutes\"\n",
    "PLACEMENT_GROUPS_KEY = \"placement_groups\"\n",
    "\n",
    "\n",
    "# Step Keys\n",
    "STEP_ID_KEY = \"step_id\"\n",
    "PROPERTIES_KEY = \"properties\"\n",
    "ACTION_ON_FAILURE_KEY = \"action_on_failure\"\n",
    "STATE_CHANGE_REASON_CODE_KEY = \"state_change_reason_code\"\n",
    "FAILURE_REASON_KEY = \"failure_reason\"\n",
    "START_DATE_TIME_KEY = \"start_date_time\"\n",
    "\n",
    "\n",
    "# InstanceGroups Keys\n",
    "MARKET_KEY = \"market\"\n",
    "INSTANCE_GROUP_TYPE_KEY = \"instance_group_type\"\n",
    "REQUESTED_INSTANCE_COUNT_KEY = \"requested_instance_count\"\n",
    "RUNNING_INSTANCE_COUNT_KEY = \"running_instance_count\"\n",
    "CONFIGURATIONS_VERSION_KEY = \"configurations_version\"\n",
    "AUTO_SCALING_POLICY_KEY = \"auto_scaling_policy\"\n",
    "EBS_BLOCK_DEVICES_KEY = \"ebs_block_devices\"\n",
    "VOLUME_TYPE_KEY = \"volume_type\"\n",
    "SIZE_IN_GB_KEY = \"size_in_gb\"\n",
    "IOPS_KEY = \"iops\"\n",
    "THROUGHPUT_KEY = \"throughput\"\n",
    "DEVICE_KEY = \"device\"\n",
    "\n",
    "\n",
    "# InstanceFleets Keys\n",
    "INSTANCE_FLEET_TYPE_KEY = \"instance_fleet_type\"\n",
    "TARGET_ON_DEMAND_CAPACITY_KEY = \"target_on_demand_capacity\"\n",
    "TARGET_SPOT_CAPACITY_KEY = \"target_spot_capacity\"\n",
    "PROVISIONED_ON_DEMAND_CAPACITY_KEY = \"provisioned_on_demand_capacity\"\n",
    "PROVISIONED_SPOT_CAPACITY_KEY = \"provisioned_spot_capacity\"\n",
    "INSTANCE_TYPE_SPECIFICATIONS_KEY = \"instance_type_specifications\"\n",
    "WEIGHTED_CAPACITY_KEY = \"weighted_capacity\"\n",
    "BID_PRICE_AS_PERCENTAGE_OF_ON_DEMAND_PRICE_KEY = \"bid_price_as_percentage_of_on_demand_price\"\n",
    "LAUNCH_SPECIFICATIONS_KEY = \"launch_specifications\"\n",
    "SPOT_SPECIFICATION_KEY = \"spot_specification\"\n",
    "TIMEOUT_DURATION_MINUTES_KEY = \"timeout_duration_minutes\"\n",
    "TIMEOUT_ACTION_KEY = \"timeout_action\"\n",
    "BLOCK_DURATION_MINUTES_KEY = \"block_duration_minutes\"\n",
    "ALLOCATION_STRATEGY_KEY = \"allocation_strategy\"\n",
    "ON_DEMAND_SPECIFICATION_KEY = \"on_demand_specification\"\n",
    "SPOT_RESIZE_SPECIFICATION_KEY = \"spot_resize_specification\"\n",
    "MIN_TARGET_CAPACITY_KEY = \"min_target_capacity\"\n",
    "MAX_TARGET_CAPACITY_KEY = \"max_target_capacity\"\n",
    "ON_DEMAND_RESIZE_SPECIFICATION_KEY = \"on_demand_resize_specification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade9818-c3bd-4665-a545-68791f0bdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Literals\n",
    "ID_LITERAL = \"Id\"\n",
    "STATUS_LITERAL = \"Status\"\n",
    "STATE_LITERAL = \"State\"\n",
    "STATE_CHANGE_REASON_LITERAL = \"StateChangeReason\"\n",
    "CODE_LITERAL = \"Code\"\n",
    "TIMELINE_LITERAL = \"Timeline\"\n",
    "CREATION_DATE_TIME_LITERAL = \"CreationDateTime\"\n",
    "READY_DATE_TIME_LITERAL = \"ReadyDateTime\"\n",
    "END_DATE_TIME_LITERAL = \"EndDateTime\"\n",
    "CONFIGURATIONS_LITERAL = \"Configurations\"\n",
    "INSTANCE_TYPE_LITERAL = \"InstanceType\"\n",
    "MARKET_LITERAL = \"Market\"\n",
    "BID_PRICE_LITERAL = \"BidPrice\"\n",
    "EBS_OPTIMIZED_LITERAL = \"EbsOptimized\"\n",
    "NA_LITERAL = \"N/A\"\n",
    "\n",
    "# Cluster Literals\n",
    "NORMALIZED_INSTANCE_HOURS_LITERAL = \"NormalizedInstanceHours\"\n",
    "AUTO_TERMINATE_LITERAL = \"AutoTerminate\"\n",
    "TERMINATION_PROTECTED_LITERAL = \"TerminationProtected\"\n",
    "PROVISIONING_TIMEOUT_MINUTES_LITERAL = \"ProvisioningTimeoutMinutes\"\n",
    "RELEASE_LABEL_LITERAL = \"ReleaseLabel\"\n",
    "SCALE_DOWN_BEHAVIOR_LITERAL = \"ScaleDownBehavior\"\n",
    "STEP_CONCURRENCY_LEVEL_LITERAL = \"StepConcurrencyLevel\"\n",
    "EBS_ROOT_VOLUME_SIZE_LITERAL = \"EbsRootVolumeSize\"\n",
    "OS_RELEASE_LABEL_LITERAL = \"OSReleaseLabel\"\n",
    "EC2_INSTANCE_ATTRIBUTES_LITERAL = \"Ec2InstanceAttributes\"\n",
    "EC2_AVAILABILITY_ZONE_LITERAL = \"Ec2AvailabilityZone\"\n",
    "APPLICATIONS_LITERAL = \"Applications\"\n",
    "MANAGED_SCALING_POLICY_LITERAL = \"ManagedScalingPolicy\"\n",
    "BOOTSTRAP_ACTIONS_LITERAL = \"BootstrapActions\"\n",
    "REDUCTIONS_LITERAL = \"Reductions\"\n",
    "MANAGED_SCALING_JOINING_TIMEOUT_MINUTES_LITERAL = \"ManagedScalingJoiningTimeoutMinutes\"\n",
    "PLACEMENT_GROUPS_LITERAL = \"PlacementGroups\"\n",
    "\n",
    "# Step Literals\n",
    "CONFIG_LITERAL = \"Config\"\n",
    "PROPERTIES_LITERAL = \"Properties\"\n",
    "ACTION_ON_FAILURE_LITERAL = \"ActionOnFailure\"\n",
    "FAILURE_DETAILS_LITERAL = \"FailureDetails\"\n",
    "REASON_LITERAL = \"Reason\"\n",
    "START_DATE_TIME_LITERAL = \"StartDateTime\"\n",
    "\n",
    "# Instance Literals\n",
    "INSTANCE_GROUP_ID = \"InstanceGroupId\"\n",
    "INSTANCE_FLEET_ID = \"InstanceFleetId\"\n",
    "INSTANCES_EC2_ID = \"Ec2InstanceId\"\n",
    "\n",
    "# InstanceGroups Literals\n",
    "INSTANCE_GROUP_TYPE_LITERAL = \"InstanceGroupType\"\n",
    "REQUESTED_INSTANCE_COUNT_LITERAL = \"RequestedInstanceCount\"\n",
    "RUNNING_INSTANCE_COUNT_LITERAL = \"RunningInstanceCount\"\n",
    "CONFIGURATIONS_VERSION_LITERAL = \"ConfigurationsVersion\"\n",
    "AUTO_SCALING_POLICY_LITERAL = \"AutoScalingPolicy\"\n",
    "EBS_BLOCK_DEVICES_LITERAL = \"EbsBlockDevices\"\n",
    "VOLUME_SPECIFICATION_LITERAL = \"VolumeSpecification\"\n",
    "VOLUME_TYPE_LITERAL = \"VolumeType\"\n",
    "SIZE_IN_GB_LITERAL = \"SizeInGB\"\n",
    "IOPS_LITERAL = \"Iops\"\n",
    "THROUGHPUT_LITERAL = \"Throughput\"\n",
    "DEVICE_LITERAL = \"Device\"\n",
    "\n",
    "# InstanceFleets Literals\n",
    "INSTANCE_FLEET_TYPE_LITERAL = \"InstanceFleetType\"\n",
    "TARGET_ON_DEMAND_CAPACITY_LITERAL = \"TargetOnDemandCapacity\"\n",
    "TARGET_SPOT_CAPACITY_LITERAL = \"TargetSpotCapacity\"\n",
    "PROVISIONED_ON_DEMAND_CAPACITY_LITERAL = \"ProvisionedOnDemandCapacity\"\n",
    "PROVISIONED_SPOT_CAPACITY_LITERAL = \"ProvisionedSpotCapacity\"\n",
    "INSTANCE_TYPE_SPECIFICATIONS_LITERAL = \"InstanceTypeSpecifications\"\n",
    "WEIGHTED_CAPACITY_LITERAL = \"WeightedCapacity\"\n",
    "BID_PRICE_AS_PERCENTAGE_OF_ON_DEMAND_PRICE_LITERAL = \"BidPriceAsPercentageOfOnDemandPrice\"\n",
    "LAUNCH_SPECIFICATIONS_LITERAL = \"LaunchSpecifications\"\n",
    "SPOT_SPECIFICATION_LITERAL = \"SpotSpecification\"\n",
    "TIMEOUT_DURATION_MINUTES_LITERAL = \"TimeoutDurationMinutes\"\n",
    "TIMEOUT_ACTION_LITERAL = \"TimeoutAction\"\n",
    "BLOCK_DURATION_MINUTES_LITERAL = \"BlockDurationMinutes\"\n",
    "ALLOCATION_STRATEGY_LITERAL = \"AllocationStrategy\"\n",
    "ON_DEMAND_SPECIFICATION_LITERAL = \"OnDemandSpecification\"\n",
    "RESIZE_SPECIFICATIONS_LITERAL = \"ResizeSpecifications\"\n",
    "SPOT_RESIZE_SPECIFICATION_LITERAL = \"SpotResizeSpecification\"\n",
    "MIN_TARGET_CAPACITY_LITERAL = \"MinTargetCapacity\"\n",
    "MAX_TARGET_CAPACITY_LITERAL = \"MaxTargetCapacity\"\n",
    "ON_DEMAND_RESIZE_SPECIFICATION_LITERAL = \"OnDemandResizeSpecification\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc992a",
   "metadata": {},
   "source": [
    "# Logging Setup\n",
    "5. This section configures the \"emr_ec2_estimator\" logger to display informational messages, warnings, and errors directly in your notebook's output, aiding in monitoring the EMR EC2 estimation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bb719",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LOG_LEVEL = logging.INFO\n",
    "\n",
    "logger = logging.getLogger(\"emr_ec2_estimator\")\n",
    "logger.setLevel(MIN_LOG_LEVEL)\n",
    "\n",
    "if not logger.handlers:\n",
    "    formatter = logging.Formatter(\n",
    "        \"{asctime} - {name} - {levelname} - {message}\",\n",
    "        style=\"{\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    stream_handler.setLevel(MIN_LOG_LEVEL)\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b507c6-1afa-4d6f-8520-7cf5677a363c",
   "metadata": {},
   "source": [
    "## EMR EC2 Estimator Manager\n",
    "\n",
    "6. Defines the **EMR Estimator Manager** classes to handle estimation logic and data:\n",
    "\n",
    "* **EMRSEstimatorManager**: The base class with the core, shared logic.\n",
    "* **EMRSEstimatorManagerDefault**: Manages data in memory and is designed for standard workloads. This data is returned as an HTML download.\n",
    "* **EMRSEstimatorManagerS3**: Manages data by writing it directly to CSV files and uploading them to an S3 bucket. This approach is specifically for large workloads that could exceed the memory limits of a standard HTML download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974925e602021004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMRSEstimatorManager:\n",
    "    \"\"\"Manage EMR EC2 clusters, steps, instances, and cost estimations.\n",
    "\n",
    "    Attributes:\n",
    "        region_name (str): AWS region name.\n",
    "        email (str): User email for identification.\n",
    "        company (str): Company name for identification.\n",
    "        execution_id (str): Unique identifier for the execution.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        region_name: str,\n",
    "        email: str,\n",
    "        company: str,\n",
    "    ):\n",
    "        \"\"\"Initialize the EMRSEstimatorManager with region, email, and company.\n",
    "\n",
    "        Args:\n",
    "            region_name (str): AWS region name.\n",
    "            email (str): User email.\n",
    "            company (str): Company name.\n",
    "\n",
    "        \"\"\"\n",
    "        self.region_name = region_name\n",
    "        self.email = email\n",
    "        self.company = company\n",
    "        self.execution_id = str(uuid.uuid4())\n",
    "        self.output_config = {\n",
    "            \"title\": \"Download Analysis Data\",\n",
    "            \"execution_detail_name_file\": \"execution_detail.csv\",\n",
    "            \"clusters_name_file\": \"clusters_list.csv\",\n",
    "            \"steps_name_file\": \"steps_list.csv\",\n",
    "            \"instances_name_file\": \"instances_list.csv\",\n",
    "            \"instance_groups_name_file\": \"instance_groups_list.csv\",\n",
    "            \"instance_fleets_name_file\": \"instance_fleets_list.csv\",\n",
    "        }\n",
    "\n",
    "    def add_cluster(self, cluster: dict):\n",
    "        \"\"\"Process and add a cluster.\n",
    "\n",
    "        Args:\n",
    "            cluster (dict): Raw cluster information.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_cluster = self.get_processed_cluster(cluster)\n",
    "            if processed_cluster:\n",
    "                self.loader_add_cluster(processed_cluster)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding cluster: %s\", e)\n",
    "\n",
    "    def add_step(self, step: dict, cluster_id: str):\n",
    "        \"\"\"Process and add a step to a cluster.\n",
    "\n",
    "        Args:\n",
    "            step (dict): Raw step information.\n",
    "            cluster_id (str): The ID of the cluster to which the step belongs.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_step = self.get_processed_step(step, cluster_id)\n",
    "            if processed_step:\n",
    "                self.loader_add_step(processed_step)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding step: %s\", e)\n",
    "\n",
    "    def add_instance(self, instance: dict, cluster_id: str):\n",
    "        \"\"\"Process and add an instance to a cluster.\n",
    "\n",
    "        Args:\n",
    "            instance (dict): Raw instance information.\n",
    "            cluster_id (str): The ID of the cluster to which the instance belongs.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_instance = self.get_processed_instance(instance, cluster_id)\n",
    "            if processed_instance:\n",
    "                self.loader_add_instance(processed_instance)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding instance: %s\", e)\n",
    "\n",
    "    def add_instance_group(self, instance_group: dict, cluster_id: str):\n",
    "        \"\"\"Process and add an instance group to a cluster.\n",
    "\n",
    "        Args:\n",
    "            instance_group (dict): Raw instance group information.\n",
    "            cluster_id (str): The ID of the cluster to which the instance group belongs.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_instance_group = self.get_processed_instance_group(\n",
    "                instance_group,\n",
    "                cluster_id,\n",
    "            )\n",
    "            if processed_instance_group:\n",
    "                self.loader_add_instance_group(processed_instance_group)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding instance group: %s\", e)\n",
    "\n",
    "    def add_instance_fleet(self, instance_fleet: dict, cluster_id: str):\n",
    "        \"\"\"Process and add an instance fleet to a cluster.\n",
    "\n",
    "        Args:\n",
    "            instance_fleet (dict): Raw instance fleet information.\n",
    "            cluster_id (str): The ID of the cluster to which the instance fleet belongs.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_instance_fleet = self.get_processed_instance_fleet(\n",
    "                instance_fleet,\n",
    "                cluster_id,\n",
    "            )\n",
    "            if processed_instance_fleet:\n",
    "                self.loader_add_instance_fleet(processed_instance_fleet)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding instance fleet: %s\", e)\n",
    "\n",
    "    def loader_add_cluster(self, processed_cluster: dict):\n",
    "        \"\"\"Implement the logic to handle a processed cluster.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement loader_add_cluster\")\n",
    "\n",
    "    def loader_add_step(self, processed_step: dict):\n",
    "        \"\"\"Implement the logic to handle a processed step.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement loader_add_step\")\n",
    "\n",
    "    def loader_add_instance(self, processed_instance: dict):\n",
    "        \"\"\"Implement the logic to handle a processed instance.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement loader_add_instance\")\n",
    "\n",
    "    def loader_add_instance_group(self, processed_instance_group: dict):\n",
    "        \"\"\"Implement the logic to handle a processed instance group.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"Subclasses must implement loader_add_instance_group\",\n",
    "        )\n",
    "\n",
    "    def loader_add_instance_fleet(self, processed_instance_fleet: dict):\n",
    "        \"\"\"Implement the logic to handle a processed instance fleet.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"Subclasses must implement loader_add_instance_fleet\",\n",
    "        )\n",
    "\n",
    "    def get_processed_cluster(self, cluster: dict) -> dict:\n",
    "        \"\"\"Process and return a formatted cluster dictionary.\n",
    "\n",
    "        This method processes raw cluster information retrieved from AWS EMR\n",
    "        and returns a dictionary containing the cluster's details in a structured format.\n",
    "\n",
    "        Args:\n",
    "            cluster (dict): Raw cluster information retrieved from AWS EMR.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing processed cluster details.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timeline_data = self.extract_timeline_data(cluster)\n",
    "            ready_time = (\n",
    "                cluster.get(STATUS_LITERAL, {}).get(TIMELINE_LITERAL, {}).get(READY_DATE_TIME_LITERAL, NA_LITERAL)\n",
    "            )\n",
    "\n",
    "            new_cluster = {\n",
    "                CLUSTER_ID_KEY: cluster.get(ID_LITERAL, NA_LITERAL),\n",
    "                STATUS_KEY: cluster.get(STATUS_LITERAL, {}).get(STATE_LITERAL, NA_LITERAL),\n",
    "                STATE_CHANGE_REASON_KEY: cluster.get(STATUS_LITERAL, {})\n",
    "                .get(STATE_CHANGE_REASON_LITERAL, {})\n",
    "                .get(CODE_LITERAL, NA_LITERAL),\n",
    "                CREATION_DATE_TIME_KEY: timeline_data.get(\n",
    "                    CREATION_DATE_TIME_KEY,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                READY_DATE_TIME_KEY: str(ready_time),\n",
    "                END_DATE_TIME_KEY: timeline_data.get(END_DATE_TIME_KEY, NA_LITERAL),\n",
    "                DURATION_SECONDS_KEY: timeline_data.get(DURATION_SECONDS_KEY, NA_LITERAL),\n",
    "                NORMALIZED_INSTANCE_HOURS_KEY: cluster.get(\n",
    "                    NORMALIZED_INSTANCE_HOURS_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                AUTO_TERMINATE_KEY: cluster.get(AUTO_TERMINATE_LITERAL, False),\n",
    "                TERMINATION_PROTECTED_KEY: cluster.get(\n",
    "                    TERMINATION_PROTECTED_LITERAL,\n",
    "                    False,\n",
    "                ),\n",
    "                PROVISIONING_TIMEOUT_MINUTES_KEY: cluster.get(\n",
    "                    PROVISIONING_TIMEOUT_MINUTES_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                RELEASE_LABEL_KEY: cluster.get(RELEASE_LABEL_LITERAL, NA_LITERAL),\n",
    "                SCALE_DOWN_BEHAVIOR_KEY: cluster.get(\n",
    "                    SCALE_DOWN_BEHAVIOR_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                STEP_CONCURRENCY_LEVEL_KEY: cluster.get(\n",
    "                    STEP_CONCURRENCY_LEVEL_LITERAL,\n",
    "                    1,\n",
    "                ),\n",
    "                EBS_ROOT_VOLUME_SIZE_KEY: cluster.get(EBS_ROOT_VOLUME_SIZE_LITERAL, 0),\n",
    "                OS_RELEASE_LABEL_KEY: cluster.get(OS_RELEASE_LABEL_LITERAL, NA_LITERAL),\n",
    "                EC2_AVAILABILITY_ZONE_KEY: cluster.get(\n",
    "                    EC2_INSTANCE_ATTRIBUTES_LITERAL,\n",
    "                    {},\n",
    "                ).get(EC2_AVAILABILITY_ZONE_LITERAL, NA_LITERAL),\n",
    "                APPLICATIONS_KEY: cluster.get(APPLICATIONS_LITERAL, NA_LITERAL),\n",
    "                MANAGED_SCALING_POLICY_KEY: cluster.get(MANAGED_SCALING_POLICY_LITERAL),\n",
    "                CONFIGURATIONS_KEY: cluster.get(CONFIGURATIONS_LITERAL, []),\n",
    "                BOOTSTRAP_ACTIONS_KEY: cluster.get(BOOTSTRAP_ACTIONS_LITERAL, []),\n",
    "                REDUCTIONS_KEY: cluster.get(REDUCTIONS_LITERAL, []),\n",
    "                MANAGED_SCALING_JOINING_TIMEOUT_MINUTES_KEY: cluster.get(\n",
    "                    MANAGED_SCALING_JOINING_TIMEOUT_MINUTES_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                PLACEMENT_GROUPS_KEY: cluster.get(PLACEMENT_GROUPS_LITERAL, []),\n",
    "            }\n",
    "            return new_cluster\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding cluster: %s\", e)\n",
    "            return None\n",
    "\n",
    "    def get_processed_step(self, step: dict, cluster_id: str) -> dict:\n",
    "        \"\"\"Process and return a formatted step dictionary.\n",
    "\n",
    "        This method processes raw step information retrieved from AWS EMR\n",
    "        and returns a dictionary containing the step's details in a structured format.\n",
    "\n",
    "        Args:\n",
    "            step (dict): Raw step information retrieved from AWS EMR.\n",
    "            cluster_id (str): The ID of the cluster to which the step belongs.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing processed step details.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timeline_data = self.extract_timeline_data(step)\n",
    "            start_time = step.get(STATUS_LITERAL, {}).get(TIMELINE_LITERAL, {}).get(START_DATE_TIME_LITERAL, NA_LITERAL)\n",
    "\n",
    "            new_step = {\n",
    "                STEP_ID_KEY: step.get(ID_LITERAL, NA_LITERAL),\n",
    "                CLUSTER_ID_KEY: cluster_id,\n",
    "                PROPERTIES_KEY: step.get(CONFIG_LITERAL, {}).get(\n",
    "                    PROPERTIES_LITERAL,\n",
    "                    {},\n",
    "                ),\n",
    "                ACTION_ON_FAILURE_KEY: step.get(ACTION_ON_FAILURE_LITERAL, NA_LITERAL),\n",
    "                STATE_KEY: step.get(STATUS_LITERAL, {}).get(\n",
    "                    STATE_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                STATE_CHANGE_REASON_CODE_KEY: step.get(STATUS_LITERAL, {})\n",
    "                .get(STATE_CHANGE_REASON_LITERAL, {})\n",
    "                .get(CODE_LITERAL, NA_LITERAL),\n",
    "                FAILURE_REASON_KEY: step.get(STATUS_LITERAL, {})\n",
    "                .get(FAILURE_DETAILS_LITERAL, {})\n",
    "                .get(REASON_LITERAL, NA_LITERAL),\n",
    "                CREATION_DATE_TIME_KEY: timeline_data.get(\n",
    "                    CREATION_DATE_TIME_KEY,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                START_DATE_TIME_KEY: str(start_time),\n",
    "                END_DATE_TIME_KEY: timeline_data.get(END_DATE_TIME_KEY, NA_LITERAL),\n",
    "                DURATION_SECONDS_KEY: timeline_data.get(DURATION_SECONDS_KEY, NA_LITERAL),\n",
    "            }\n",
    "            return new_step\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding step: %s\", e)\n",
    "            return None\n",
    "\n",
    "    def get_processed_instance(self, instance: dict, cluster_id: str) -> dict:\n",
    "        \"\"\"Process and return a formatted instance dictionary.\n",
    "\n",
    "        This method processes raw instance information retrieved from AWS EMR\n",
    "        and returns a dictionary containing the instance's details in a structured format.\n",
    "\n",
    "        Args:\n",
    "            instance (dict): Raw instance information retrieved from AWS EMR.\n",
    "            cluster_id (str): The ID of the cluster to which the instance belongs.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing processed instance details.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timeline_data = self.extract_timeline_data(instance)\n",
    "            ready_time = (\n",
    "                instance.get(STATUS_LITERAL, {}).get(TIMELINE_LITERAL, {}).get(READY_DATE_TIME_LITERAL, NA_LITERAL)\n",
    "            )\n",
    "\n",
    "            new_instance = {\n",
    "                INSTANCE_ID_KEY: instance.get(INSTANCES_EC2_ID, NA_LITERAL),\n",
    "                CLUSTER_ID_KEY: cluster_id,\n",
    "                MARKET_KEY: instance.get(MARKET_LITERAL, NA_LITERAL),\n",
    "                INSTANCE_GROUP_TYPE_KEY: instance.get(INSTANCE_GROUP_ID, NA_LITERAL),\n",
    "                INSTANCE_FLEET_TYPE_KEY: instance.get(INSTANCE_FLEET_ID, NA_LITERAL),\n",
    "                INSTANCE_TYPE_KEY: instance.get(INSTANCE_TYPE_LITERAL, NA_LITERAL),\n",
    "                STATE_KEY: instance.get(STATUS_LITERAL, {}).get(STATE_LITERAL, NA_LITERAL),\n",
    "                CODE_KEY: instance.get(STATUS_LITERAL, {})\n",
    "                .get(STATE_CHANGE_REASON_LITERAL, {})\n",
    "                .get(CODE_LITERAL, NA_LITERAL),\n",
    "                CREATION_DATE_TIME_KEY: timeline_data.get(\n",
    "                    CREATION_DATE_TIME_KEY,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                READY_DATE_TIME_KEY: str(ready_time),\n",
    "                END_DATE_TIME_KEY: timeline_data.get(END_DATE_TIME_KEY, NA_LITERAL),\n",
    "                DURATION_SECONDS_KEY: timeline_data.get(DURATION_SECONDS_KEY, NA_LITERAL),\n",
    "            }\n",
    "            return new_instance\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding instance: %s\", e)\n",
    "            return None\n",
    "\n",
    "    def get_processed_instance_group(self, instance_group: dict, cluster_id: str) -> dict:\n",
    "        \"\"\"Process and return a formatted instance group dictionary.\n",
    "\n",
    "        This method processes raw instance group information retrieved from AWS EMR\n",
    "        and returns a dictionary containing the instance group's details in a structured format.\n",
    "\n",
    "        Args:\n",
    "            instance_group (dict): Raw instance group information retrieved from AWS EMR.\n",
    "            cluster_id (str): The ID of the cluster to which the instance group belongs.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing processed instance group details.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            timeline_data = self.extract_timeline_data(instance_group)\n",
    "            ready_time = (\n",
    "                instance_group.get(STATUS_LITERAL, {})\n",
    "                .get(TIMELINE_LITERAL, {})\n",
    "                .get(READY_DATE_TIME_LITERAL, NA_LITERAL)\n",
    "            )\n",
    "\n",
    "            new_instance_group = {\n",
    "                INSTANCE_GROUP_ID_KEY: instance_group.get(ID_LITERAL, NA_LITERAL),\n",
    "                CLUSTER_ID_KEY: cluster_id,\n",
    "                MARKET_KEY: instance_group.get(MARKET_LITERAL, NA_LITERAL),\n",
    "                INSTANCE_GROUP_TYPE_KEY: instance_group.get(\n",
    "                    INSTANCE_GROUP_TYPE_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                INSTANCE_TYPE_KEY: instance_group.get(INSTANCE_TYPE_LITERAL, NA_LITERAL),\n",
    "                BID_PRICE_KEY: instance_group.get(BID_PRICE_LITERAL, NA_LITERAL),\n",
    "                EBS_OPTIMIZED_KEY: instance_group.get(EBS_OPTIMIZED_LITERAL, False),\n",
    "                REQUESTED_INSTANCE_COUNT_KEY: instance_group.get(\n",
    "                    REQUESTED_INSTANCE_COUNT_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                RUNNING_INSTANCE_COUNT_KEY: instance_group.get(\n",
    "                    RUNNING_INSTANCE_COUNT_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                STATE_KEY: instance_group.get(STATUS_LITERAL, {}).get(\n",
    "                    STATE_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                CODE_KEY: instance_group.get(STATUS_LITERAL, {})\n",
    "                .get(STATE_CHANGE_REASON_LITERAL, {})\n",
    "                .get(CODE_LITERAL, NA_LITERAL),\n",
    "                CREATION_DATE_TIME_KEY: timeline_data.get(\n",
    "                    CREATION_DATE_TIME_KEY,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                READY_DATE_TIME_KEY: str(ready_time),\n",
    "                END_DATE_TIME_KEY: timeline_data.get(END_DATE_TIME_KEY, NA_LITERAL),\n",
    "                DURATION_SECONDS_KEY: timeline_data.get(DURATION_SECONDS_KEY, NA_LITERAL),\n",
    "                CONFIGURATIONS_KEY: instance_group.get(CONFIGURATIONS_LITERAL, []),\n",
    "                CONFIGURATIONS_VERSION_KEY: instance_group.get(\n",
    "                    CONFIGURATIONS_VERSION_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                AUTO_SCALING_POLICY_KEY: instance_group.get(\n",
    "                    AUTO_SCALING_POLICY_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                EBS_BLOCK_DEVICES_KEY: [\n",
    "                    {\n",
    "                        VOLUME_TYPE_KEY: device.get(\n",
    "                            VOLUME_SPECIFICATION_LITERAL,\n",
    "                            {},\n",
    "                        ).get(VOLUME_TYPE_LITERAL, NA_LITERAL),\n",
    "                        SIZE_IN_GB_KEY: device.get(\n",
    "                            VOLUME_SPECIFICATION_LITERAL,\n",
    "                            {},\n",
    "                        ).get(SIZE_IN_GB_LITERAL, 0),\n",
    "                        IOPS_KEY: device.get(VOLUME_SPECIFICATION_LITERAL, {}).get(\n",
    "                            IOPS_LITERAL,\n",
    "                            NA_LITERAL,\n",
    "                        ),\n",
    "                        THROUGHPUT_KEY: device.get(\n",
    "                            VOLUME_SPECIFICATION_LITERAL,\n",
    "                            {},\n",
    "                        ).get(THROUGHPUT_LITERAL, NA_LITERAL),\n",
    "                        DEVICE_KEY: device.get(DEVICE_LITERAL, NA_LITERAL),\n",
    "                    }\n",
    "                    for device in instance_group.get(EBS_BLOCK_DEVICES_LITERAL, [])\n",
    "                ],\n",
    "            }\n",
    "            return new_instance_group\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding instance group: %s\", e)\n",
    "            return None\n",
    "\n",
    "    def get_processed_instance_fleet(self, instance_fleet: dict, cluster_id: str) -> dict:\n",
    "        \"\"\"Process and return a formatted instance fleet dictionary.\n",
    "\n",
    "        This method processes raw instance fleet information retrieved from AWS EMR\n",
    "        and returns a dictionary containing the instance fleet's details in a structured format.\n",
    "\n",
    "        Args:\n",
    "            instance_fleet (dict): Raw instance fleet information retrieved from AWS EMR.\n",
    "            cluster_id (str): The ID of the cluster to which the instance fleet belongs.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing processed instance fleet details.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            new_instance_fleet = {\n",
    "                INSTANCE_FLEET_ID_KEY: instance_fleet.get(ID_LITERAL, NA_LITERAL),\n",
    "                CLUSTER_ID_KEY: cluster_id,\n",
    "                INSTANCE_FLEET_TYPE_KEY: instance_fleet.get(\n",
    "                    INSTANCE_FLEET_TYPE_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                STATE_KEY: instance_fleet.get(STATUS_LITERAL, {}).get(\n",
    "                    STATE_LITERAL,\n",
    "                    NA_LITERAL,\n",
    "                ),\n",
    "                CODE_KEY: instance_fleet.get(STATUS_LITERAL, {})\n",
    "                .get(STATE_CHANGE_REASON_LITERAL, {})\n",
    "                .get(CODE_LITERAL, NA_LITERAL),\n",
    "                CREATION_DATE_TIME_KEY: str(\n",
    "                    instance_fleet.get(STATUS_LITERAL, {})\n",
    "                    .get(TIMELINE_LITERAL, {})\n",
    "                    .get(CREATION_DATE_TIME_LITERAL, NA_LITERAL),\n",
    "                ),\n",
    "                READY_DATE_TIME_KEY: str(\n",
    "                    instance_fleet.get(STATUS_LITERAL, {})\n",
    "                    .get(TIMELINE_LITERAL, {})\n",
    "                    .get(READY_DATE_TIME_LITERAL, NA_LITERAL),\n",
    "                ),\n",
    "                END_DATE_TIME_KEY: str(\n",
    "                    instance_fleet.get(STATUS_LITERAL, {})\n",
    "                    .get(TIMELINE_LITERAL, {})\n",
    "                    .get(END_DATE_TIME_LITERAL, NA_LITERAL),\n",
    "                ),\n",
    "                TARGET_ON_DEMAND_CAPACITY_KEY: instance_fleet.get(\n",
    "                    TARGET_ON_DEMAND_CAPACITY_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                TARGET_SPOT_CAPACITY_KEY: instance_fleet.get(\n",
    "                    TARGET_SPOT_CAPACITY_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                PROVISIONED_ON_DEMAND_CAPACITY_KEY: instance_fleet.get(\n",
    "                    PROVISIONED_ON_DEMAND_CAPACITY_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                PROVISIONED_SPOT_CAPACITY_KEY: instance_fleet.get(\n",
    "                    PROVISIONED_SPOT_CAPACITY_LITERAL,\n",
    "                    0,\n",
    "                ),\n",
    "                INSTANCE_TYPE_SPECIFICATIONS_KEY: [\n",
    "                    {\n",
    "                        INSTANCE_TYPE_KEY: spec.get(INSTANCE_TYPE_LITERAL, NA_LITERAL),\n",
    "                        WEIGHTED_CAPACITY_KEY: spec.get(WEIGHTED_CAPACITY_LITERAL, 0),\n",
    "                        BID_PRICE_KEY: spec.get(BID_PRICE_LITERAL, NA_LITERAL),\n",
    "                        BID_PRICE_AS_PERCENTAGE_OF_ON_DEMAND_PRICE_KEY: spec.get(\n",
    "                            BID_PRICE_AS_PERCENTAGE_OF_ON_DEMAND_PRICE_LITERAL,\n",
    "                            0.0,\n",
    "                        ),\n",
    "                        EBS_OPTIMIZED_KEY: spec.get(EBS_OPTIMIZED_LITERAL, False),\n",
    "                        CONFIGURATIONS_KEY: spec.get(CONFIGURATIONS_LITERAL, []),\n",
    "                        SPOT_SPECIFICATION_KEY + \"_\" + TIMEOUT_DURATION_MINUTES_KEY: spec.get(\n",
    "                            LAUNCH_SPECIFICATIONS_LITERAL,\n",
    "                            {},\n",
    "                        )\n",
    "                        .get(SPOT_SPECIFICATION_LITERAL, {})\n",
    "                        .get(TIMEOUT_DURATION_MINUTES_LITERAL, 0),\n",
    "                        SPOT_SPECIFICATION_KEY + \"_\" + TIMEOUT_ACTION_KEY: spec.get(\n",
    "                            LAUNCH_SPECIFICATIONS_LITERAL,\n",
    "                            {},\n",
    "                        )\n",
    "                        .get(SPOT_SPECIFICATION_LITERAL, {})\n",
    "                        .get(TIMEOUT_ACTION_LITERAL, NA_LITERAL),\n",
    "                        SPOT_SPECIFICATION_KEY + \"_\" + BLOCK_DURATION_MINUTES_KEY: spec.get(\n",
    "                            LAUNCH_SPECIFICATIONS_LITERAL,\n",
    "                            {},\n",
    "                        )\n",
    "                        .get(SPOT_SPECIFICATION_LITERAL, {})\n",
    "                        .get(BLOCK_DURATION_MINUTES_LITERAL, NA_LITERAL),\n",
    "                        SPOT_SPECIFICATION_KEY + \"_\" + ALLOCATION_STRATEGY_KEY: spec.get(\n",
    "                            LAUNCH_SPECIFICATIONS_LITERAL,\n",
    "                            {},\n",
    "                        )\n",
    "                        .get(SPOT_SPECIFICATION_LITERAL, {})\n",
    "                        .get(ALLOCATION_STRATEGY_LITERAL, NA_LITERAL),\n",
    "                        ON_DEMAND_SPECIFICATION_KEY + \"_\" + ALLOCATION_STRATEGY_KEY: spec.get(\n",
    "                            LAUNCH_SPECIFICATIONS_LITERAL,\n",
    "                            {},\n",
    "                        )\n",
    "                        .get(ON_DEMAND_SPECIFICATION_LITERAL, {})\n",
    "                        .get(ALLOCATION_STRATEGY_LITERAL, NA_LITERAL),\n",
    "                    }\n",
    "                    for spec in instance_fleet.get(\n",
    "                        INSTANCE_TYPE_SPECIFICATIONS_LITERAL,\n",
    "                        [],\n",
    "                    )\n",
    "                ],\n",
    "                SPOT_RESIZE_SPECIFICATION_KEY + \"_\" + TIMEOUT_DURATION_MINUTES_KEY: instance_fleet.get(\n",
    "                    RESIZE_SPECIFICATIONS_LITERAL,\n",
    "                    {},\n",
    "                )\n",
    "                .get(SPOT_RESIZE_SPECIFICATION_LITERAL, {})\n",
    "                .get(TIMEOUT_DURATION_MINUTES_LITERAL, 0),\n",
    "                SPOT_RESIZE_SPECIFICATION_KEY + \"_\" + MIN_TARGET_CAPACITY_KEY: instance_fleet.get(\n",
    "                    RESIZE_SPECIFICATIONS_LITERAL,\n",
    "                    {},\n",
    "                )\n",
    "                .get(SPOT_RESIZE_SPECIFICATION_LITERAL, {})\n",
    "                .get(MIN_TARGET_CAPACITY_LITERAL, 0),\n",
    "                SPOT_RESIZE_SPECIFICATION_KEY + \"_\" + MAX_TARGET_CAPACITY_KEY: instance_fleet.get(\n",
    "                    RESIZE_SPECIFICATIONS_LITERAL,\n",
    "                    {},\n",
    "                )\n",
    "                .get(SPOT_RESIZE_SPECIFICATION_LITERAL, {})\n",
    "                .get(MAX_TARGET_CAPACITY_LITERAL, 0),\n",
    "                ON_DEMAND_RESIZE_SPECIFICATION_KEY + \"_\" + TIMEOUT_DURATION_MINUTES_KEY: instance_fleet.get(\n",
    "                    RESIZE_SPECIFICATIONS_LITERAL,\n",
    "                    {},\n",
    "                )\n",
    "                .get(ON_DEMAND_RESIZE_SPECIFICATION_LITERAL, {})\n",
    "                .get(TIMEOUT_DURATION_MINUTES_LITERAL, 0),\n",
    "                ON_DEMAND_RESIZE_SPECIFICATION_KEY + \"_\" + MIN_TARGET_CAPACITY_KEY: instance_fleet.get(\n",
    "                    RESIZE_SPECIFICATIONS_LITERAL,\n",
    "                    {},\n",
    "                )\n",
    "                .get(ON_DEMAND_RESIZE_SPECIFICATION_LITERAL, {})\n",
    "                .get(MIN_TARGET_CAPACITY_LITERAL, 0),\n",
    "                ON_DEMAND_RESIZE_SPECIFICATION_KEY + \"_\" + MAX_TARGET_CAPACITY_KEY: instance_fleet.get(\n",
    "                    RESIZE_SPECIFICATIONS_LITERAL,\n",
    "                    {},\n",
    "                )\n",
    "                .get(ON_DEMAND_RESIZE_SPECIFICATION_LITERAL, {})\n",
    "                .get(MAX_TARGET_CAPACITY_LITERAL, 0),\n",
    "            }\n",
    "            return new_instance_fleet\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error adding instance fleet: %s\", e)\n",
    "            return None\n",
    "\n",
    "    def get_execution_info(self) -> dict:\n",
    "        \"\"\"Retrieve execution metadata.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing execution metadata.\n",
    "\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"execution_id\": self.execution_id,\n",
    "            \"email\": self.email,\n",
    "            \"company\": self.company,\n",
    "            \"region\": self.region_name,\n",
    "            \"version\": NOTEBOOK_VERSION,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            \"runs_for_last_days\": runs_for_last_days,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_timeline_data(data: dict) -> dict:\n",
    "        \"\"\"Extract timeline data from a given dictionary.\n",
    "\n",
    "        Args:\n",
    "            data (dict): Dictionary containing timeline information.\n",
    "\n",
    "        Returns:\n",
    "            dict: Extracted timeline data including creation, end times, and duration.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            creation_time = data.get(STATUS_LITERAL, {}).get(TIMELINE_LITERAL, {}).get(CREATION_DATE_TIME_LITERAL, None)\n",
    "            end_time = data.get(STATUS_LITERAL, {}).get(TIMELINE_LITERAL, {}).get(END_DATE_TIME_LITERAL, None)\n",
    "            duration = 0\n",
    "            if isinstance(creation_time, datetime) and isinstance(end_time, datetime):\n",
    "                duration = (end_time - creation_time).total_seconds()\n",
    "            return {\n",
    "                CREATION_DATE_TIME_KEY: str(creation_time or NA_LITERAL),\n",
    "                END_DATE_TIME_KEY: str(end_time or NA_LITERAL),\n",
    "                DURATION_SECONDS_KEY: duration,\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error extracting timeline data: %s\", e)\n",
    "            return {}\n",
    "\n",
    "\n",
    "class EMRSEstimatorManagerDefault(EMRSEstimatorManager):\n",
    "    \"\"\"Manage EMR EC2 clusters, steps, instances, and cost estimations.\n",
    "\n",
    "    Attributes:\n",
    "        region_name (str): AWS region name.\n",
    "        email (str): User email for identification.\n",
    "        company (str): Company name for identification.\n",
    "        execution_id (str): Unique identifier for the execution.\n",
    "        clusters (list): List of clusters information.\n",
    "        steps (list): List of steps information.\n",
    "        instances (list): List of instances information.\n",
    "        instance_groups (list): List of instance groups information.\n",
    "        instance_fleets (list): List of instance fleets information.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        region_name: str,\n",
    "        email: str,\n",
    "        company: str,\n",
    "    ):\n",
    "        \"\"\"Initialize the EMRSEstimatorManagerEC2 with region, email, and company.\n",
    "\n",
    "        Args:\n",
    "            region_name (str): AWS region name.\n",
    "            email (str): User email.\n",
    "            company (str): Company name.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(region_name, email, company)\n",
    "        self.clusters = []\n",
    "        self.steps = []\n",
    "        self.instance_groups = []\n",
    "        self.instance_fleets = []\n",
    "        self.instances = []\n",
    "\n",
    "    def loader_add_cluster(self, processed_cluster: dict):\n",
    "        \"\"\"Add a processed cluster to the clusters list.\"\"\"\n",
    "        self.clusters.append(processed_cluster)\n",
    "\n",
    "    def loader_add_step(self, processed_step: dict):\n",
    "        \"\"\"Add a processed step to the steps list.\"\"\"\n",
    "        self.steps.append(processed_step)\n",
    "\n",
    "    def loader_add_instance(self, processed_instance: dict):\n",
    "        \"\"\"Add a processed instance to the instances list.\"\"\"\n",
    "        self.instances.append(processed_instance)\n",
    "\n",
    "    def loader_add_instance_group(self, processed_instance_group: dict):\n",
    "        \"\"\"Add a processed instance group to the instance groups list.\"\"\"\n",
    "        self.instance_groups.append(processed_instance_group)\n",
    "\n",
    "    def loader_add_instance_fleet(self, processed_instance_fleet: dict):\n",
    "        \"\"\"Add a processed instance fleet to the instance fleets list.\"\"\"\n",
    "        self.instance_fleets.append(processed_instance_fleet)\n",
    "\n",
    "    def show_output(self, output_file_name: str = None):\n",
    "        \"\"\"Generate and display a downloadable ZIP file containing analysis data.\n",
    "\n",
    "        Args:\n",
    "            output_file_name (str, optional): Name of the output ZIP file. Defaults to None.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if output_file_name is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_file_name = f\"emr_ec2_we_output_{timestamp}.zip\"\n",
    "\n",
    "            clusters_df = pd.DataFrame(self.clusters)\n",
    "            clusters_buffer = self.get_buffer_output(clusters_df)\n",
    "\n",
    "            steps_df = pd.DataFrame(self.steps)\n",
    "            steps_buffer = self.get_buffer_output(steps_df)\n",
    "\n",
    "            instances_df = pd.DataFrame(self.instances)\n",
    "            instances_buffer = self.get_buffer_output(instances_df)\n",
    "\n",
    "            instance_groups_df = pd.DataFrame(self.instance_groups)\n",
    "            instance_groups_buffer = self.get_buffer_output(instance_groups_df)\n",
    "\n",
    "            instance_fleets_df = pd.DataFrame(self.instance_fleets)\n",
    "            instance_fleets_buffer = self.get_buffer_output(instance_fleets_df)\n",
    "\n",
    "            total_clusters = clusters_df.shape[0]\n",
    "            total_steps = steps_df.shape[0]\n",
    "            total_instances = instances_df.shape[0]\n",
    "            total_instance_groups = instance_groups_df.shape[0]\n",
    "            total_instance_fleets = instance_fleets_df.shape[0]\n",
    "\n",
    "            execution_info_df = pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        **self.get_execution_info(),\n",
    "                        \"total_clusters\": total_clusters,\n",
    "                        \"total_steps\": total_steps,\n",
    "                        \"total_instances\": total_instances,\n",
    "                        \"total_instance_groups\": total_instance_groups,\n",
    "                        \"total_instance_fleets\": total_instance_fleets,\n",
    "                    },\n",
    "                ],\n",
    "            )\n",
    "            execution_info_buffer = self.get_buffer_output(execution_info_df)\n",
    "\n",
    "            payload = self.compress_data(\n",
    "                execution_info_buffer,\n",
    "                clusters_buffer,\n",
    "                steps_buffer,\n",
    "                instances_buffer,\n",
    "                instance_groups_buffer,\n",
    "                instance_fleets_buffer,\n",
    "            )\n",
    "\n",
    "            html = (\n",
    "                f'<html><div style=\"display:flex;justify-content: center;\">'\n",
    "                f'<a download=\"{output_file_name}\" '\n",
    "                f'href=\"data:application/zip;base64,{payload}\" '\n",
    "                f'target=\"_blank\">'\n",
    "                f'<button style=\"background-color:#249edc;color: #fff;'\n",
    "                f\"border:1px solid #249edc;cursor:pointer;border-radius:45px;\"\n",
    "                f'font-weight:800;line-height:18px;padding: 8px 16px\" '\n",
    "                f'type=\"button\">{self.output_config.get(\"title\")}</button>'\n",
    "                f\"</a></div></html>\"\n",
    "            )\n",
    "            display(HTML(html))\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error showing output: %s\", e)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_buffer_output(data: pd.DataFrame) -> io.StringIO:\n",
    "        \"\"\"Convert a DataFrame to a CSV buffer.\n",
    "\n",
    "        Args:\n",
    "            data (pd.DataFrame): DataFrame to convert.\n",
    "\n",
    "        Returns:\n",
    "            io.StringIO: Buffer containing the CSV data.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            buffer = io.StringIO()\n",
    "            data.to_csv(buffer, index=False, encoding=\"utf-8\")\n",
    "            buffer.seek(0)\n",
    "            return buffer\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error getting buffer output: %s\", e)\n",
    "\n",
    "    def compress_data(\n",
    "        self,\n",
    "        execution_info_buffer: io.StringIO,\n",
    "        clusters_buffer: io.StringIO,\n",
    "        steps_buffer: io.StringIO,\n",
    "        instances_buffer: io.StringIO,\n",
    "        instance_groups_buffer: io.StringIO,\n",
    "        instance_fleets_buffer: io.StringIO,\n",
    "    ) -> str:\n",
    "        \"\"\"Compress data into a ZIP file and encode it in base64.\n",
    "\n",
    "        Args:\n",
    "            execution_info_buffer (io.StringIO): Buffer containing execution info.\n",
    "            clusters_buffer (io.StringIO): Buffer containing clusters data.\n",
    "            steps_buffer (io.StringIO): Buffer containing steps data.\n",
    "            instances_buffer (io.StringIO): Buffer containing instances data.\n",
    "            instance_groups_buffer (io.StringIO): Buffer containing instance groups data.\n",
    "            instance_fleets_buffer (io.StringIO): Buffer containing instance fleets data.\n",
    "\n",
    "        Returns:\n",
    "            str: Base64-encoded ZIP file content.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            files_to_compress = {\n",
    "                self.output_config.get(\n",
    "                    \"execution_detail_name_file\",\n",
    "                ): execution_info_buffer,\n",
    "                self.output_config.get(\"clusters_name_file\"): clusters_buffer,\n",
    "                self.output_config.get(\"steps_name_file\"): steps_buffer,\n",
    "                self.output_config.get(\"instances_name_file\"): instances_buffer,\n",
    "                self.output_config.get(\n",
    "                    \"instance_groups_name_file\",\n",
    "                ): instance_groups_buffer,\n",
    "                self.output_config.get(\n",
    "                    \"instance_fleets_name_file\",\n",
    "                ): instance_fleets_buffer,\n",
    "            }\n",
    "            zip_buffer = io.BytesIO()\n",
    "            with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "                for file_name, buffer in files_to_compress.items():\n",
    "                    if file_name and buffer and buffer.getvalue():\n",
    "                        zf.writestr(file_name, buffer.getvalue())\n",
    "                    elif file_name:\n",
    "                        logger.warning(\n",
    "                            \"Empty buffer for file: %s. It will not be added to the ZIP.\",\n",
    "                            file_name,\n",
    "                        )\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            \"File name not provided for one of the buffers in output_config. It will be skipped.\",\n",
    "                        )\n",
    "\n",
    "            zip_bytes = zip_buffer.getvalue()\n",
    "            b64 = base64.b64encode(zip_bytes).decode()\n",
    "            return b64\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error compressing data: %s\", e)\n",
    "            return None\n",
    "\n",
    "\n",
    "class EMRSEstimatorManagerS3(EMRSEstimatorManager):\n",
    "    \"\"\"Manage EMR EC2 clusters, steps, instances, and cost estimations.\n",
    "\n",
    "    Attributes:\n",
    "        region_name (str): AWS region name.\n",
    "        email (str): User email for identification.\n",
    "        company (str): Company name for identification.\n",
    "        execution_id (str): Unique identifier for the execution.\n",
    "        clusters (list): List of clusters information.\n",
    "        steps (list): List of steps information.\n",
    "        instances (list): List of instances information.\n",
    "        instance_groups (list): List of instance groups information.\n",
    "        instance_fleets (list): List of instance fleets information.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        region_name: str,\n",
    "        email: str,\n",
    "        company: str,\n",
    "        output: str = f\"{os.getcwd()}/.out_emr_we\",\n",
    "    ):\n",
    "        \"\"\"Initialize the EMRSEstimatorManagerEC2 with region, email, and company.\n",
    "\n",
    "        Args:\n",
    "            region_name (str): AWS region name.\n",
    "            email (str): User email.\n",
    "            company (str): Company name.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(region_name, email, company)\n",
    "        self.total_clusters = 0\n",
    "        self.total_steps = 0\n",
    "        self.total_instances = 0\n",
    "        self.total_instance_groups = 0\n",
    "        self.total_instance_fleets = 0\n",
    "        self.output = output\n",
    "        if os.path.isdir(output):\n",
    "            shutil.rmtree(output)\n",
    "        os.makedirs(output, exist_ok=True)\n",
    "\n",
    "    def loader_add_cluster(self, processed_cluster: dict):\n",
    "        \"\"\"Write a processed cluster to a CSV file and increment the total count.\"\"\"\n",
    "        self.write_in_csv(\n",
    "            processed_cluster,\n",
    "            self.output_config.get(\"clusters_name_file\"),\n",
    "        )\n",
    "        self.total_clusters += 1\n",
    "\n",
    "    def loader_add_step(self, processed_step: dict):\n",
    "        \"\"\"Write a processed step to a CSV file and increment the total count.\"\"\"\n",
    "        self.write_in_csv(\n",
    "            processed_step,\n",
    "            self.output_config.get(\"steps_name_file\"),\n",
    "        )\n",
    "        self.total_steps += 1\n",
    "\n",
    "    def loader_add_instance(self, processed_instance: dict):\n",
    "        \"\"\"Write a processed instance to a CSV file and increment the total count.\"\"\"\n",
    "        self.write_in_csv(\n",
    "            processed_instance,\n",
    "            self.output_config.get(\"instances_name_file\"),\n",
    "        )\n",
    "        self.total_instances += 1\n",
    "\n",
    "    def loader_add_instance_group(self, processed_instance_group: dict):\n",
    "        \"\"\"Write a processed instance group to a CSV file and increment the total count.\"\"\"\n",
    "        self.write_in_csv(\n",
    "            processed_instance_group,\n",
    "            self.output_config.get(\"instance_groups_name_file\"),\n",
    "        )\n",
    "        self.total_instance_groups += 1\n",
    "\n",
    "    def loader_add_instance_fleet(self, processed_instance_fleet: dict):\n",
    "        \"\"\"Write a processed instance fleet to a CSV file and increment the total count.\"\"\"\n",
    "        self.write_in_csv(\n",
    "            processed_instance_fleet,\n",
    "            self.output_config.get(\"instance_fleets_name_file\"),\n",
    "        )\n",
    "        self.total_instance_fleets += 1\n",
    "\n",
    "    def compress_folder_to_zip(self) -> io.BytesIO:\n",
    "        \"\"\"Compress the output folder into a ZIP file.\n",
    "\n",
    "        This method compresses all files in the output folder into a ZIP file and\n",
    "        returns the compressed data as a `BytesIO` object.\n",
    "\n",
    "        Returns:\n",
    "            io.BytesIO: A buffer containing the compressed ZIP file data.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.output):\n",
    "                raise Exception(f\"The source folder '{self.output}' does not exist.\")\n",
    "            zip_buffer = io.BytesIO()\n",
    "            logger.info(\"Beginning to add files to the ZIP...\")\n",
    "            with zipfile.ZipFile(zip_buffer, \"w\", zipfile.ZIP_DEFLATED) as zipf:\n",
    "                for root, dirs, files in os.walk(self.output):\n",
    "                    for file in files:\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        relative_path = os.path.relpath(file_path, self.output)\n",
    "                        zipf.write(file_path, relative_path)\n",
    "            logger.info(\"Compression complete.\")\n",
    "            return zip_buffer\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error compressing folder %s\", e)\n",
    "            return None\n",
    "\n",
    "    def write_in_csv(self, response: dict, file_name: str):\n",
    "        \"\"\"Write a dictionary response to a CSV file.\n",
    "\n",
    "        This method appends the given dictionary response to a CSV file. If the file\n",
    "        does not exist, it creates a new one with the appropriate headers.\n",
    "\n",
    "        Args:\n",
    "            response (dict): The dictionary data to write to the CSV file.\n",
    "            file_name (str): The name of the CSV file to write to.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.DataFrame([response])\n",
    "            output_file_path = os.path.join(self.output, file_name)\n",
    "            if os.path.exists(output_file_path):\n",
    "                df.to_csv(output_file_path, mode=\"a\", header=False, index=False)\n",
    "            else:\n",
    "                df.to_csv(output_file_path, mode=\"w\", header=True, index=False)\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error writing in CSV: %s\", e)\n",
    "\n",
    "    def save_in_s3(self, output_file_name: str = None):\n",
    "        \"\"\"Save the output data to an S3 bucket.\n",
    "\n",
    "        This method compresses the output data into a ZIP file and uploads it to the specified S3 bucket.\n",
    "        It also writes execution metadata to a CSV file before compression.\n",
    "\n",
    "        Args:\n",
    "            output_file_name (str, optional): The name of the ZIP file to be uploaded.\n",
    "                If not provided, a default name is generated using the current timestamp.\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if output_file_name is None:\n",
    "                timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "                output_file_name = f\"emr_ec2_we_output_{timestamp}.zip\"\n",
    "\n",
    "            execution_info = {\n",
    "                **self.get_execution_info(),\n",
    "                \"total_clusters\": self.total_clusters,\n",
    "                \"total_steps\": self.total_steps,\n",
    "                \"total_instances\": self.total_instances,\n",
    "                \"total_instance_groups\": self.total_instance_groups,\n",
    "                \"total_instance_fleets\": self.total_instance_fleets,\n",
    "            }\n",
    "            self.write_in_csv(\n",
    "                execution_info,\n",
    "                self.output_config.get(\"execution_detail_name_file\"),\n",
    "            )\n",
    "\n",
    "            zip_buffer = self.compress_folder_to_zip()\n",
    "            s3_key = f\"{s3_folder_name}/{output_file_name}\"\n",
    "            if zip_buffer:\n",
    "                zip_buffer.seek(0)\n",
    "                s3_client.upload_fileobj(zip_buffer, s3_bucket_name, s3_key)\n",
    "                logger.info(\"File uploaded to S3 successfully.\")\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Error saving output: %s\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5224f4ed9ca393",
   "metadata": {},
   "source": [
    "# Utility functions\n",
    "\n",
    "7. Defines utility functions for retrieving data from AWS (applications, job runs, costs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eba44b4f0de5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_user_info() -> bool:\n",
    "    \"\"\"Validate the user information required for the application.\n",
    "\n",
    "    Checks if the region, email, company, and the number of days for job runs\n",
    "    are properly configured. Logs errors or warnings for invalid or missing values.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the user information is valid, False otherwise.\n",
    "\n",
    "    \"\"\"\n",
    "    is_valid = True\n",
    "    if region_name == \"\":\n",
    "        logger.error(\"Region is empty. Please provide a valid region.\")\n",
    "        is_valid = False\n",
    "    if not isinstance(runs_for_last_days, int) or runs_for_last_days < 1:\n",
    "        logger.error(\"Runs for last days must be a positive integer.\")\n",
    "        is_valid = False\n",
    "    if email == \"\":\n",
    "        logger.warning(\"Email is empty. Please provide a valid email.\")\n",
    "    if company == \"\":\n",
    "        logger.warning(\"Company is empty. Please provide a valid company.\")\n",
    "\n",
    "    return is_valid\n",
    "\n",
    "\n",
    "class InstanceCollectionType(Enum):\n",
    "    \"\"\"Enumeration for instance collection types.\n",
    "\n",
    "    Attributes:\n",
    "        GROUP (int): Represents instance groups.\n",
    "        FLEET (int): Represents instance fleets.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    GROUP = 1\n",
    "    FLEET = 2\n",
    "\n",
    "\n",
    "def list_clusters(\n",
    "    now: datetime,\n",
    "    runs_for_last_days_ago: datetime,\n",
    ") -> Iterator[str, InstanceCollectionType]:\n",
    "    \"\"\"Retrieve a list of EMR clusters within a specified date range.\n",
    "\n",
    "    Args:\n",
    "        now (datetime): Current date and time.\n",
    "        runs_for_last_days_ago (datetime): Start date for retrieving clusters.\n",
    "\n",
    "    Yields:\n",
    "        Iterator[str, InstanceCollectionType]: Cluster ID and instance collection type.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paginator = emr_ec2_client.get_paginator(\"list_clusters\")\n",
    "        response_iterator = paginator.paginate(\n",
    "            CreatedAfter=runs_for_last_days_ago,\n",
    "            CreatedBefore=now,\n",
    "            ClusterStates=[\"TERMINATING\", \"TERMINATED\", \"TERMINATED_WITH_ERRORS\"],\n",
    "        )\n",
    "\n",
    "        for page in response_iterator:\n",
    "            if \"Clusters\" in page:\n",
    "                for cluster in page[\"Clusters\"]:\n",
    "                    cluster_id = cluster[\"Id\"]\n",
    "                    cluster_info = emr_ec2_client.describe_cluster(\n",
    "                        ClusterId=cluster_id,\n",
    "                    )\n",
    "                    instance_collection_type = InstanceCollectionType.FLEET\n",
    "                    if cluster_info[\"Cluster\"][\"InstanceCollectionType\"] == \"INSTANCE_GROUP\":\n",
    "                        instance_collection_type = InstanceCollectionType.GROUP\n",
    "                    estimator_manager.add_cluster(cluster_info[\"Cluster\"])\n",
    "                    yield cluster_id, instance_collection_type\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error listing clusters: %s\", e)\n",
    "\n",
    "\n",
    "def list_steps(cluster_id: str):\n",
    "    \"\"\"Retrieve a list of steps for a given EMR cluster.\n",
    "\n",
    "    Args:\n",
    "        cluster_id (str): ID of the cluster to retrieve steps for.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paginator = emr_ec2_client.get_paginator(\"list_steps\")\n",
    "        response_iterator = paginator.paginate(ClusterId=cluster_id)\n",
    "        for page in response_iterator:\n",
    "            if \"Steps\" in page:\n",
    "                for step in page[\"Steps\"]:\n",
    "                    step_id = step[\"Id\"]\n",
    "                    step_info = emr_ec2_client.describe_step(\n",
    "                        ClusterId=cluster_id,\n",
    "                        StepId=step_id,\n",
    "                    )\n",
    "                    estimator_manager.add_step(step_info[\"Step\"], cluster_id)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error listing steps: %s\", e)\n",
    "\n",
    "\n",
    "def list_all_instances(cluster_id: str):\n",
    "    \"\"\"Retrieve a list of all instances for a given EMR cluster.\n",
    "\n",
    "    Args:\n",
    "        cluster_id (str): ID of the cluster to retrieve instances for.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paginator = emr_ec2_client.get_paginator(\"list_instances\")\n",
    "        response_iterator = paginator.paginate(ClusterId=cluster_id)\n",
    "        for page in response_iterator:\n",
    "            if \"Instances\" in page:\n",
    "                for instances in page[\"Instances\"]:\n",
    "                    if instances.get(\"Market\", \"\") == \"ON_DEMAND\":\n",
    "                        estimator_manager.add_instance(instances, cluster_id)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error listing instances: %s\", e)\n",
    "\n",
    "\n",
    "def list_instances_type(\n",
    "    cluster_id: str,\n",
    "    instance_collection_type: InstanceCollectionType,\n",
    "):\n",
    "    \"\"\"Retrieve instance groups or fleets based on the cluster's instance collection type.\n",
    "\n",
    "    Args:\n",
    "        cluster_id (str): ID of the cluster to retrieve instances for.\n",
    "        instance_collection_type (InstanceCollectionType): Type of instance collection (GROUP or FLEET).\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if instance_collection_type == InstanceCollectionType.GROUP:\n",
    "            list_instance_groups(cluster_id)\n",
    "        elif instance_collection_type == InstanceCollectionType.FLEET:\n",
    "            list_instance_fleets(cluster_id)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error listing instances_types: %s\", e)\n",
    "\n",
    "\n",
    "def list_instance_groups(cluster_id: str):\n",
    "    \"\"\"Retrieve a list of instance groups for a given EMR cluster.\n",
    "\n",
    "    Args:\n",
    "        cluster_id (str): ID of the cluster to retrieve instance groups for.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paginator = emr_ec2_client.get_paginator(\"list_instance_groups\")\n",
    "        response_iterator = paginator.paginate(ClusterId=cluster_id)\n",
    "        for page in response_iterator:\n",
    "            if \"InstanceGroups\" in page:\n",
    "                for instance_group in page[\"InstanceGroups\"]:\n",
    "                    if instance_group.get(\"Market\") == \"ON_DEMAND\":\n",
    "                        estimator_manager.add_instance_group(instance_group, cluster_id)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error listing instance groups: %s\", e)\n",
    "\n",
    "\n",
    "def list_instance_fleets(cluster_id: str):\n",
    "    \"\"\"Retrieve a list of instance fleets for a given EMR cluster.\n",
    "\n",
    "    Args:\n",
    "        cluster_id (str): ID of the cluster to retrieve instance fleets for.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        paginator = emr_ec2_client.get_paginator(\"list_instance_fleets\")\n",
    "        response_iterator = paginator.paginate(ClusterId=cluster_id)\n",
    "        for page in response_iterator:\n",
    "            if \"InstanceFleets\" in page:\n",
    "                for instance_fleet in page[\"InstanceFleets\"]:\n",
    "                    estimator_manager.add_instance_fleet(instance_fleet, cluster_id)\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error listing instance fleets: %s\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a819d-0115-4961-a903-e99f5bc59db2",
   "metadata": {},
   "source": [
    "# Generate estimation\n",
    "\n",
    "8. Executes the main estimation workflow: instantiates the manager, retrieves data, and generates the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3b26a-2338-49b5-8154-0246f2a8904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if validate_user_info():\n",
    "    # Initialize the EMR client with the specified region and configuration\n",
    "    emr_ec2_client = boto3.client(\"emr\", region_name=region_name, config=config)\n",
    "\n",
    "    is_s3_option = s3_bucket_name and s3_folder_name\n",
    "\n",
    "    if is_s3_option:\n",
    "        s3_client = boto3.client(\"s3\", region_name=region_name, config=config)\n",
    "        estimator_manager = EMRSEstimatorManagerS3(region_name, email, company)\n",
    "    else:\n",
    "        estimator_manager = EMRSEstimatorManagerDefault(region_name, email, company)\n",
    "    now = datetime.now(timezone.utc)\n",
    "\n",
    "    # Calculate the start date for retrieving clusters based on the configured number of days\n",
    "    runs_for_last_days_ago = now - timedelta(days=runs_for_last_days)\n",
    "\n",
    "    logger.info(\"Start date: %s\", runs_for_last_days_ago.strftime(\"%Y-%m-%d\"))\n",
    "    logger.info(\"End date: %s\", now.strftime(\"%Y-%m-%d\"))\n",
    "    logger.info(\"Starting to extract the information: this process may take a few minutes...\")\n",
    "\n",
    "    # Iterate through the clusters retrieved within the specified date range\n",
    "    for cluster_id, instance_collection_type in list_clusters(\n",
    "        now,\n",
    "        runs_for_last_days_ago,\n",
    "    ):\n",
    "        # Retrieve and add steps associated with the cluster\n",
    "        list_steps(cluster_id)\n",
    "        # Retrieve and add all instances associated with the cluster\n",
    "        list_all_instances(cluster_id)\n",
    "        # Retrieve and add instance groups or fleets based on the cluster's instance collection type\n",
    "        list_instances_type(cluster_id, instance_collection_type)\n",
    "\n",
    "    if is_s3_option:\n",
    "        # The output is saved directly to the configured bucket.\n",
    "        logger.info(\"Total clusters: %d\", estimator_manager.total_clusters)\n",
    "        logger.info(\"Total steps: %d\", estimator_manager.total_steps)\n",
    "        logger.info(\"Total instances: %d\", estimator_manager.total_instances)\n",
    "        logger.info(\n",
    "            \"Total instance groups: %d\",\n",
    "            estimator_manager.total_instance_groups,\n",
    "        )\n",
    "        logger.info(\n",
    "            \"Total instance fleets: %d\",\n",
    "            estimator_manager.total_instance_fleets,\n",
    "        )\n",
    "        estimator_manager.save_in_s3()\n",
    "    else:\n",
    "        # Generate and display a downloadable ZIP file containing the analysis data\n",
    "        logger.info(\"Total clusters: %d\", len(estimator_manager.clusters))\n",
    "        logger.info(\"Total steps: %d\", len(estimator_manager.steps))\n",
    "        logger.info(\"Total instances: %d\", len(estimator_manager.instances))\n",
    "        logger.info(\"Total instance groups: %d\", len(estimator_manager.instance_groups))\n",
    "        logger.info(\"Total instance fleets: %d\", len(estimator_manager.instance_fleets))\n",
    "        estimator_manager.show_output()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
